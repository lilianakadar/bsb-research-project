{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pycaret.classification import setup, compare_models\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the target and feature combo for which test metrics is gatheres\n",
    "\n",
    "itarget = \"bcl\"         #options: bcl, mcl   \n",
    "feature = 'md'          #fp, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train data\n",
    "\n",
    "file_name = f'../../data_preparation/variable_{feature}_generation/inhibitors_{feature}/inhibitors_{itarget}_{feature}_sub.csv'\n",
    "df = pd.read_csv(file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"] = df[\"Class\"].replace({'Inhibitor':1, 'Non-inhibitor':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "df_train = df.drop(['Unnamed: 0','papyrus_SMILES','InChIKey','connectivity','pchembl_value_Mean'], axis=1)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define evaluation metrics: balanced accuracy, balanced MCC\n",
    "\n",
    "#Function for balanced accuracy\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "#Function for balanced MCC\n",
    "def balanced_mcc(y_true, y_pred):\n",
    "    # Get confusion matrix components\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate sensitivity, specificity, and prevalence\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    positive_prevalence = (TP + FN) / (TP + FP + TN + FN)\n",
    "    \n",
    "    # Calculate Balanced MCC\n",
    "    numerator = sensitivity + specificity - 1\n",
    "    denominator = np.sqrt(\n",
    "        (sensitivity + (1-specificity) * ((1-positive_prevalence) / positive_prevalence )) * \n",
    "        (specificity + (1-sensitivity) * (positive_prevalence / (1-positive_prevalence)))\n",
    "    )\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    elif numerator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['XGBClassifier', 'AdaBoostClassifier', 'DecisionTreeClassifier', 'DummyClassifier', 'ExtraTreesClassifier','GaussianNB','GradientBoostingClassifier','KNeighborsClassifier','LGBMClassifier',\n",
    "              'LinearDiscriminantAnalysis','LogisticRegression','QuadraticDiscriminantAnalysis','RandomForestClassifier','RidgeClassifier','SGDClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids=[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"random\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "# Loop through the session IDs\n",
    "for model in model_names:\n",
    "    print(f'Prediction with model: {model}')\n",
    "    for s_id in session_ids:\n",
    "        print(f\"Setting up PyCaret session: {s_id} -{feature} - {itarget}\")\n",
    "\n",
    "        # Setup the environment with the specific session ID\n",
    "        grid= setup(df, \n",
    "                    target=\"Class\", \n",
    "                    session_id=s_id, \n",
    "                    html=True, \n",
    "                    verbose=False, \n",
    "                    fold=5, \n",
    "                    data_split_shuffle=True, \n",
    "                    remove_multicollinearity=True, \n",
    "                    multicollinearity_threshold=0.9, \n",
    "                    low_variance_threshold=0.05)\n",
    "    \n",
    "        # Add the custom metrics to pycaret\n",
    "        add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "        add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "        #add_metric('enrichment_factor_score','Enrichment Factor',_enrichment_factor_score, target='pred_proba')\n",
    "        #add_metric('bedroc_score','BEDROC',_bedroc_score, target='pred_proba')\n",
    "\n",
    "        class_model_file = f'models/inhibitors_{itarget}_{split}_{feature}_session_{s_id}'\n",
    "\n",
    "        class_model = load_model(class_model_file)\n",
    "        \n",
    "        predictions = predict_model(class_model, raw_score=True)\n",
    "\n",
    "     # Collect metrics for this session and model\n",
    "        session_metrics = pull()\n",
    "        \n",
    "        # Add model and session information to the metrics\n",
    "        session_metrics['Model'] = model\n",
    "        \n",
    "        all_metrics.append(session_metrics)\n",
    "\n",
    "# Create a DataFrame from the list of metrics\n",
    "metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "#Save the metrics\n",
    "metrics_filename = f\"metrics/{itarget}_{split}_test_raw_metrics_predictions_{feature}.csv\"\n",
    "metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'temporal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train and test data\n",
    "file_name_train = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_train.csv'\n",
    "df_train = pd.read_csv(file_name_train, index_col=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "file_name_test = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_test.csv'\n",
    "df_test = pd.read_csv(file_name_test,index_col=0)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe cleaning for train\n",
    "\n",
    "columns_to_drop = ['papyrus_SMILES', 'InChIKey', 'inchi_connectivity','pchembl_value_Mean']\n",
    "df = df_train.drop(columns=columns_to_drop)\n",
    "testing = df_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification fix\n",
    "df['Class'] = df['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "\n",
    "testing['Class'] = testing['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "testing['Class'] = testing['Class'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Loop through the session IDs\n",
    "for model in model_names:\n",
    "    print(f'Prediction with model: {model}')\n",
    "    for s_id in session_ids:\n",
    "        print(f\"Setting up PyCaret session: {s_id} -{feature} - {itarget} - {split}\")\n",
    "\n",
    "            # Setup the environment with the specific session ID\n",
    "        grid = setup(data=df, \n",
    "                target='Class', \n",
    "                session_id=16,\n",
    "                html=True, \n",
    "                verbose=True, \n",
    "                fold=5, \n",
    "                remove_multicollinearity=True,  \n",
    "                multicollinearity_threshold=0.9, \n",
    "                low_variance_threshold=0.05,\n",
    "                test_data= testing,\n",
    "                index=False\n",
    "        )\n",
    "    \n",
    "        # Add the custom metrics to pycaret\n",
    "        add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "        add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "        #add_metric('enrichment_factor_score','Enrichment Factor',_enrichment_factor_score, target='pred_proba')\n",
    "        #add_metric('bedroc_score','BEDROC',_bedroc_score, target='pred_proba')\n",
    "\n",
    "        class_model_file = f'models/inhibitors_{itarget}_{split}_{feature}_session_{s_id}'\n",
    "\n",
    "        class_model = load_model(class_model_file)\n",
    "        \n",
    "        predictions = predict_model(class_model, raw_score=True)\n",
    "\n",
    "     # Collect metrics for this session and model\n",
    "        session_metrics = pull()\n",
    "        \n",
    "        # Add model and session information to the metrics\n",
    "        session_metrics['Model'] = model\n",
    "        \n",
    "        all_metrics.append(session_metrics)\n",
    "\n",
    "# Create a DataFrame from the list of metrics\n",
    "metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "\n",
    "metrics_filename = f\"metrics/{itarget}_{split}_test_raw_metrics_predictions_{feature}.csv\"\n",
    "metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train and test data\n",
    "file_name_train = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_train.csv'\n",
    "df_train = pd.read_csv(file_name_train, index_col=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "file_name_test = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_test.csv'\n",
    "df_test = pd.read_csv(file_name_test,index_col=0)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe cleaning for train\n",
    "\n",
    "columns_to_drop = ['papyrus_SMILES', 'InChIKey', 'inchi_connectivity','pchembl_value_Mean']\n",
    "df = df_train.drop(columns=columns_to_drop)\n",
    "testing = df_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification fix\n",
    "df['Class'] = df['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "\n",
    "testing['Class'] = testing['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "testing['Class'] = testing['Class'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_metrics = []\n",
    "\n",
    "# Loop through the session IDs\n",
    "for model in model_names:\n",
    "    print(f'Prediction with model: {model}')\n",
    "    for s_id in session_ids:\n",
    "        print(f\"Setting up PyCaret session: {s_id} -{feature} - {itarget} - {split}\")\n",
    "\n",
    "            # Setup the environment with the specific session ID\n",
    "        grid = setup(data=df, \n",
    "                target='Class', \n",
    "                session_id=16,\n",
    "                html=True, \n",
    "                verbose=True, \n",
    "                fold=5, \n",
    "                remove_multicollinearity=True,  \n",
    "                multicollinearity_threshold=0.9, \n",
    "                low_variance_threshold=0.05,\n",
    "                test_data= testing,\n",
    "                index=False\n",
    "        )\n",
    "    \n",
    "        # Add the custom metrics to pycaret\n",
    "        add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "        add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "        #add_metric('enrichment_factor_score','Enrichment Factor',_enrichment_factor_score, target='pred_proba')\n",
    "        #add_metric('bedroc_score','BEDROC',_bedroc_score, target='pred_proba')\n",
    "\n",
    "        class_model_file = f'models/inhibitors_{itarget}_{split}_{feature}_session_{s_id}'\n",
    "\n",
    "        class_model = load_model(class_model_file)\n",
    "        \n",
    "        predictions = predict_model(class_model, raw_score=True)\n",
    "\n",
    "     # Collect metrics for this session and model\n",
    "        session_metrics = pull()\n",
    "        \n",
    "        # Add model and session information to the metrics\n",
    "        session_metrics['Model'] = model\n",
    "        \n",
    "        all_metrics.append(session_metrics)\n",
    "\n",
    "# Create a DataFrame from the list of metrics\n",
    "metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "\n",
    "metrics_filename = f\"metrics/{itarget}_{split}_test_raw_metrics_predictions_{feature}.csv\"\n",
    "metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(itarget,split,feature):\n",
    "    \n",
    "    file_name = f'metrics/{itarget}_{split}_test_raw_metrics_predictions_{feature}.csv'\n",
    "    print(f'Read file: {file_name}')\n",
    "\n",
    "    df_raw = pd.read_csv(file_name, index_col=0)\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe by MCC values\n",
    "\n",
    "def sorting(df_raw):\n",
    "    print('Sorting')\n",
    "    return df_raw.sort_values(by='MCC',  ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Transport and Feature column to know which rows belong to which dataset\n",
    "\n",
    "def add_info_column(df,itarget,feature,split):\n",
    "\n",
    "    if feature == \"md\":\n",
    "        df['Feature'] = 'MD'\n",
    "    elif feature == \"fp\":\n",
    "        df['Feature'] =\"FP\"\n",
    "    \n",
    "    print('Feature info addded')\n",
    "\n",
    "    if itarget == \"bcl\":\n",
    "        df['Target'] = \"Bcl-2\"\n",
    "    elif itarget== \"mcl\":\n",
    "        df['Target'] = \"Mcl-1\"\n",
    "\n",
    "    print('Target info added')\n",
    "\n",
    "    if split ==\"random\":\n",
    "        df['Splitting'] = \"Random\"\n",
    "    elif split ==\"temporal\":\n",
    "        df[\"Splitting\"] = \"Temporal\"\n",
    "    elif split ==\"cluster\":\n",
    "        df[\"Spliting\"] = \"Cluster\"\n",
    "\n",
    "    print('Spliting info added')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the top3 performing models and their metrics\n",
    "\n",
    "def top3(df_sorted):\n",
    "    print('Keep only top 3')\n",
    "    return df_sorted.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the sorted, filtered dataframe\n",
    "\n",
    "def get_df(itarget, feature):\n",
    "    return top3(add_info_column(sorting(read_file(itarget,feature)),itarget,feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the different target, features and splitting\n",
    "\n",
    "targets= ['mcl','bcl']\n",
    "features = ['md','fp']\n",
    "splitting_methods =[\"random\",\"temporal\",'cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine each target-feature-split combination into one dataset, descending by MCC\n",
    "\n",
    "for split in splitting_methods:\n",
    "    all_test = pd.DataFrame()\n",
    "    for itarget in targets:\n",
    "        for feature in features:\n",
    "            all_test = pd.concat([all_test, add_info_column(sorting(read_file(itarget,split,feature)),itarget,feature,split)], ignore_index=True)\n",
    "\n",
    "    file_name =f\"metrics/{split}_all_target_test_metrics_2.csv\"\n",
    "    all_test.to_csv(file_name, index=True)\n",
    "    print(f'{split} - {itarget} - {feature}: {len(all_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only top3 model, combine metrics\n",
    "\n",
    "for split in splitting_methods:\n",
    "    top3_test = pd.DataFrame()\n",
    "    for itarget in targets:\n",
    "        for feature in features:\n",
    "            top3_test = pd.concat([top3_test,top3(add_info_column(sorting(read_file(itarget,split,feature)),itarget,feature,split))], ignore_index=True)\n",
    "\n",
    "    file_name =f\"metrics/{split}_top3_models_target_test_metrics_2.csv\"\n",
    "    top3_test.to_csv( file_name, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splitting_methods:\n",
    "\n",
    "    read_file_name =f\"metrics/{split}_top3_models_target_test_metrics_2.csv\"\n",
    "    df = pd.read_csv(read_file_name, index_col=0)\n",
    "\n",
    "    df['Feature'] = df['Feature'].replace({'MD': 'Selected molecular descriptors', 'FP': 'Selected molecular fingerprints',})\n",
    "\n",
    "    #Visualize Top3\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(2, 3)) \n",
    "    plt.xlim(0, 1)\n",
    "\n",
    "    horizontal_lines = [0, 1]\n",
    "    desired_order = [\"Selected molecular descriptors\",'Selected molecular fingerprints']\n",
    "    custom_colors = ['#4472C4','#FFC000']\n",
    "\n",
    "    # Add horizontal guidelines\n",
    "    for val in horizontal_lines:\n",
    "        plt.axhline(val, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "  # Adjust marker size\n",
    "    ax.set(ylabel=\"\")\n",
    "\n",
    "    ax.minorticks_on()\n",
    "    ax.tick_params(which=\"both\", bottom=True)\n",
    "\n",
    "\n",
    "    plt.title('Inihibitor models performance with cluster split', fontsize=11)\n",
    "\n",
    "    if split ==\"random\": \n",
    "        ax = sns.swarmplot(data=df, x=\"MCC\", y=\"Target\", hue=\"Feature\", size=8, palette=custom_colors, hue_order=desired_order, legend=False) \n",
    "        plt.title('Inihibitor models performance with random split', fontsize=11)\n",
    "        plt.text(-0.32, -0.65, \"(A)\", fontsize=12, fontweight='bold')\n",
    "    elif split ==\"temporal\":\n",
    "        ax = sns.swarmplot(data=df, x=\"MCC\", y=\"Target\", hue=\"Feature\", size=8, palette=custom_colors, hue_order=desired_order, legend=False) \n",
    "        plt.title('Inihibitor models performance with temporal split', fontsize=11)\n",
    "        plt.text(-0.32, -0.65, \"(B)\", fontsize=12, fontweight='bold')\n",
    "    elif split ==\"cluster\":\n",
    "        ax = sns.swarmplot(data=df, x=\"MCC\", y=\"Target\", hue=\"Feature\", size=8, palette=custom_colors, hue_order=desired_order, legend=True) \n",
    "        plt.title('Inihibitor models performance with cluster split', fontsize=11)\n",
    "        plt.text(-0.32, -0.65, \"(C)\", fontsize=12, fontweight='bold')\n",
    "        plt.legend(bbox_to_anchor=(0.41, -0.38), loc='upper center', ncol=1, frameon=False)\n",
    "\n",
    "\n",
    "    plt.gcf().set_size_inches(3, 1.5)\n",
    "\n",
    "    fig_name=f'inhibitor_{split}.svg'\n",
    "    plt.savefig(fig_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
