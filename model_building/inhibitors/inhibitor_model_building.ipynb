{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pycaret.classification import setup, compare_models\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the type of classifiers to build\n",
    "\n",
    "model_list = ['et','ada','lr','ridge','gbc','rf','dt','lightgbm','svm','lda','knn','nb','qda','dummy','xgboost']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define random seed(s)\n",
    "\n",
    "session_ids=[16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define evaluation metrics: balanced accuracy, balanced MCC\n",
    "\n",
    "#Function for balanced accuracy\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "#Function for balanced MCC\n",
    "def balanced_mcc(y_true, y_pred):\n",
    "    # Get confusion matrix components\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate sensitivity, specificity, and prevalence\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    positive_prevalence = (TP + FN) / (TP + FP + TN + FN)\n",
    "    \n",
    "    # Calculate Balanced MCC\n",
    "    numerator = sensitivity + specificity - 1\n",
    "    denominator = np.sqrt(\n",
    "        (sensitivity + (1-specificity) * ((1-positive_prevalence) / positive_prevalence )) * \n",
    "        (specificity + (1-sensitivity) * (positive_prevalence / (1-positive_prevalence)))\n",
    "    )\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    elif numerator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change variables to define which dataset is used for training (compound set, target label, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itarget = \"bcl\"         #bcl or mcl\n",
    "feature = 'fp'         #md or fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and load training set (here the variables are used to define the file path that are later used for defining the saved file names as well)\n",
    "\n",
    "file_name = f'../../data_preparation/variable_{feature}_generation/inhibitors_{feature}/inhibitors_{itarget}_{feature}_sub.csv'\n",
    "df = pd.read_csv(file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dataset for cleaning\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code classes\n",
    "\n",
    "df[\"Class\"] = df[\"Class\"].replace({'Inhibitor':1, 'Non-inhibitor':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "df_train = df.drop(['Unnamed: 0','papyrus_SMILES','InChIKey','connectivity','pchembl_value_Mean'], axis=1)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build 15 classifiers with all random seed\n",
    "\n",
    "for s_id in session_ids:\n",
    "    print(f\"Setting up PyCaret session:  ID-{s_id}, Transport - {itarget}\")\n",
    "    target_col =\"Class\"\n",
    "\n",
    "    # Setup the environment with the specific session ID\n",
    "    grid = setup(data=df_train, \n",
    "             target=target_col, \n",
    "             session_id=s_id,\n",
    "             html=True, \n",
    "             verbose=True, \n",
    "             fold=5, \n",
    "             data_split_shuffle=True,\n",
    "             remove_multicollinearity=True,  \n",
    "             multicollinearity_threshold=0.9, \n",
    "             low_variance_threshold=0.05,\n",
    "    )\n",
    "    \n",
    "\n",
    "    #Get unprocessed training\n",
    "    train_raw = get_config('X_train')\n",
    "    train_raw_inchi = pd.merge(train_raw, df[[\"connectivity\"]], left_index=True, right_index=True, how='left')\n",
    "    train_raw_file_name = f'experiments/{itarget}_{feature}_raw_train_16.csv'\n",
    "    train_raw_inchi.to_csv(train_raw_file_name, index=False)\n",
    "\n",
    "\n",
    "     #Get unprocessed test\n",
    "    test_raw = get_config('X_test')\n",
    "    test_raw_inchi = pd.merge(test_raw, df[['connectivity',target_col,'papyrus_SMILES']], left_index=True, right_index=True, how='left')\n",
    "    test_raw_file_name = f'experiments/{itarget}_{feature}_raw_test_16.csv'\n",
    "    test_raw_inchi.to_csv(test_raw_file_name, index=False)\n",
    "\n",
    "\n",
    "    # Add the custom metrics to pycaret\n",
    "    add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "    add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "    \n",
    "\n",
    "    # Comparing all models \n",
    "    models_comparison = compare_models(sort=\"Balanced MCC\", n_select=16, exclude='catboost')\n",
    "    \n",
    "    #Saving all models\n",
    "    for model in models_comparison:\n",
    "        model_name = f\"models/{itarget}/{itarget}_random_{feature}_{model.__class__.__name__}_session_{s_id}\"\n",
    "        save_model(model, model_name)\n",
    "        \n",
    "       \n",
    "    # Save comparison metrics for each session as a CSV\n",
    "    metrics_df = pull()\n",
    "    metrics_filename = f\"metrics/{itarget}_random_{feature}_train_raw_metrics_session_{s_id}.csv\"\n",
    "    metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"temporal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train and test data\n",
    "file_name_train = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_train.csv'\n",
    "df_train = pd.read_csv(file_name_train, index_col=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "file_name_test = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_test.csv'\n",
    "df_test = pd.read_csv(file_name_test,index_col=0)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe cleaning for train\n",
    "\n",
    "columns_to_drop = ['papyrus_SMILES', 'InChIKey', 'inchi_connectivity','pchembl_value_Mean']\n",
    "df = df_train.drop(columns=columns_to_drop)\n",
    "testing = df_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification fix\n",
    "df['Class'] = df['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "\n",
    "testing['Class'] = testing['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "testing['Class'] = testing['Class'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_id in session_ids:\n",
    "    print(f\"Setting up PyCaret session:  ID-{s_id}, Feature -{feature}, Target - {itarget}, Split - {split}\" )\n",
    "\n",
    "    # Setup the environment with the specific session ID\n",
    "    grid = setup(data=df, \n",
    "             target='Class', \n",
    "             session_id=16,\n",
    "             html=True, \n",
    "             verbose=True, \n",
    "             fold=5, \n",
    "             remove_multicollinearity=True,  \n",
    "             multicollinearity_threshold=0.9, \n",
    "             low_variance_threshold=0.05,\n",
    "             test_data= testing,\n",
    "             index=False\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # Add the custom metrics to pycaret\n",
    "    add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "    add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "    \n",
    "\n",
    "    # Comparing all models \n",
    "    models_comparison = compare_models(sort=\"Balanced MCC\", n_select=16, exclude='catboost')\n",
    "    \n",
    "    #Saving all models\n",
    "    for model in models_comparison:\n",
    "        model_name = f\"models/{itarget}/{itarget}_{split}_{feature}_{model.__class__.__name__}_session_{s_id}\"\n",
    "        save_model(model, model_name)\n",
    "        \n",
    "       \n",
    "    # Save comparison metrics for each session as a CSV\n",
    "    metrics_df = pull()\n",
    "    metrics_filename = f\"metrics/{itarget}_{split}_{feature}_train_raw_metrics_session_{s_id}.csv\"\n",
    "    metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the train and test data\n",
    "file_name_train = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_train.csv'\n",
    "df_train = pd.read_csv(file_name_train, index_col=0)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "file_name_test = f'splitted_data/inhibitors_{itarget}_{split}_{feature}_test.csv'\n",
    "df_test = pd.read_csv(file_name_test,index_col=0)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe cleaning for train\n",
    "\n",
    "columns_to_drop = ['papyrus_SMILES', 'InChIKey', 'inchi_connectivity','pchembl_value_Mean']\n",
    "df = df_train.drop(columns=columns_to_drop)\n",
    "testing = df_test.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification fix\n",
    "df['Class'] = df['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "\n",
    "testing['Class'] = testing['Class'].replace({'Inhibitor': 1, 'Non-inhibitor': 0})\n",
    "testing['Class'] = testing['Class'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_id in session_ids:\n",
    "    print(f\"Setting up PyCaret session:  ID-{s_id}, Feature -{feature}, Target - {itarget}, Split - {split}\" )\n",
    "\n",
    "    # Setup the environment with the specific session ID\n",
    "    grid = setup(data=df, \n",
    "             target='Class', \n",
    "             session_id=16,\n",
    "             html=True, \n",
    "             verbose=True, \n",
    "             fold=5, \n",
    "             remove_multicollinearity=True,  \n",
    "             multicollinearity_threshold=0.9, \n",
    "             low_variance_threshold=0.05,\n",
    "             test_data= testing,\n",
    "             index=False\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # Add the custom metrics to pycaret\n",
    "    add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "    add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "    \n",
    "\n",
    "    # Comparing all models \n",
    "    models_comparison = compare_models(sort=\"Balanced MCC\", n_select=16, exclude='catboost')\n",
    "    \n",
    "    #Saving all models\n",
    "    for model in models_comparison:\n",
    "        model_name = f\"models/{itarget}/{itarget}_{split}_{feature}_{model.__class__.__name__}_session_{s_id}\"\n",
    "        save_model(model, model_name)\n",
    "        \n",
    "       \n",
    "    # Save comparison metrics for each session as a CSV\n",
    "    metrics_df = pull()\n",
    "    metrics_filename = f\"metrics/{itarget}_{split}_{feature}_train_raw_metrics_session_{s_id}.csv\"\n",
    "    metrics_df.to_csv(metrics_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
