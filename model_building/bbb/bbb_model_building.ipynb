{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pycaret.classification import setup, compare_models\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the type of classifiers to build\n",
    "\n",
    "model_list = ['et','ada','lr','ridge','gbc','rf','dt','lightgbm','svm','lda','knn','nb','qda','dummy','xgboost']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define random seed(s)\n",
    "\n",
    "#session_ids=[3,42,121,198]\n",
    "session_ids=[16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define evaluation metrics: balanced accuracy, balanced MCC\n",
    "\n",
    "#Function for balanced accuracy\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "#Function for balanced MCC\n",
    "def balanced_mcc(y_true, y_pred):\n",
    "    # Get confusion matrix components\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate sensitivity, specificity, and prevalence\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    positive_prevalence = (TP + FN) / (TP + FP + TN + FN)\n",
    "    \n",
    "    # Calculate Balanced MCC\n",
    "    numerator = sensitivity + specificity - 1\n",
    "    denominator = np.sqrt(\n",
    "        (sensitivity + (1-specificity) * ((1-positive_prevalence) / positive_prevalence )) * \n",
    "        (specificity + (1-sensitivity) * (positive_prevalence / (1-positive_prevalence)))\n",
    "    )\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    elif numerator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change variables to define which dataset is used for training (compound set, target label, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"kadar\"       #options: kadar, combined\n",
    "transport = \"influx\"    #options: influx, efflux, pampa, bbb\n",
    "feature = \"md\"          #options: md or fp\n",
    "feature_set = \"all\"     #options: all or sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and load training set (here the variables are used to define the file path that are later used for defining the saved file names as well)\n",
    "file_name = f'../../data_preparation/variable_{feature}_generation/{dataset}_{feature}/{dataset}_train_{transport}_{feature}_{feature_set}.csv'\n",
    "df = pd.read_csv(file_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dataset for cleaning\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target column, and code classes\n",
    "\n",
    "stat_col=f\"status_{transport}\"\n",
    "\n",
    "df.rename(columns={\"Classification\":stat_col}, inplace=True)\n",
    "\n",
    "if transport == \"influx\":\n",
    "    df[stat_col] = df[stat_col].replace({'Substrate':1, 'Non-substrate':0})\n",
    "elif transport == \"efflux\":\n",
    "    df[stat_col] = df[stat_col].replace({'Substrate':1, 'Non-substrate':0})\n",
    "elif transport == \"pampa\":\n",
    "    df[stat_col] = df[stat_col].replace({'high':1, 'low':0})\n",
    "elif transport == \"bbb\":\n",
    "    df[stat_col] = df[stat_col].replace({'BBB+':1, 'BBB-':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "if transport == \"pampa\":\n",
    "    df_train = df.drop(['Unnamed: 0','Phenotype','Permeability','SMILES_raw','papyrus_SMILES','papyrus_inchi_key','inchi_connectivity'], axis=1)\n",
    "else:\n",
    "    df_train = df.drop(['Unnamed: 0','SMILES_raw','papyrus_SMILES','papyrus_inchi_key','inchi_connectivity'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build 15 classifiers with all random seed\n",
    "\n",
    "for s_id in session_ids:\n",
    "    print(f\"Setting up PyCaret session:  ID-{s_id}, Transport - {transport}\")\n",
    "    target_col =f\"status_{transport}\"\n",
    "\n",
    "    # Setup the environment with the specific session ID\n",
    "    grid = setup(data=df_train, \n",
    "             target=target_col, \n",
    "             session_id=s_id,\n",
    "             html=True, \n",
    "             verbose=True, \n",
    "             fold=5, \n",
    "             data_split_shuffle=True,\n",
    "             remove_multicollinearity=True,  \n",
    "             multicollinearity_threshold=0.9, \n",
    "             low_variance_threshold=0.05,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    #Get unprocessed training\n",
    "    train_raw = get_config('X_train')\n",
    "    train_raw_inchi = pd.merge(train_raw, df[[\"inchi_connectivity\"]], left_index=True, right_index=True, how='left')\n",
    "    train_raw_file_name = f'{dataset}/experiments/{dataset}_{feature}_{feature_set}_{transport}_raw_train_16.csv'\n",
    "    train_raw_inchi.to_csv(train_raw_file_name, index=False)\n",
    "\n",
    "     #Get unprocessed test\n",
    "    test_raw = get_config('X_test')\n",
    "    test_raw_inchi = pd.merge(test_raw, df[[\"inchi_connectivity\",target_col,'papyrus_SMILES']], left_index=True, right_index=True, how='left')\n",
    "    test_raw_file_name = f'{dataset}/experiments/{dataset}_{feature}_{feature_set}_{transport}_raw_test_16.csv'\n",
    "    test_raw_inchi.to_csv(test_raw_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Add the custom metrics to pycaret\n",
    "    add_metric('balanced_acc', 'Balanced Accuracy', balanced_accuracy, target='pred')\n",
    "    add_metric('balanced_mcc', 'Balanced MCC', balanced_mcc, greater_is_better=True, target='pred')\n",
    "    \n",
    "\n",
    "    # Comparing all models \n",
    "    models_comparison = compare_models(sort=\"Balanced MCC\", n_select=16, exclude='catboost')\n",
    "    \n",
    "    #Saving all models\n",
    "    for model in models_comparison:\n",
    "        if feature == \"maccs\":\n",
    "            model_name = f\"models_maccs_only/{transport}/{dataset}_{transport}_maccs_{model.__class__.__name__}_session_{s_id}\"\n",
    "        else:\n",
    "            model_name = f\"models_{feature_set}_{feature}/{transport}/{dataset}_{transport}__{feature}_{feature_set}_{model.__class__.__name__}_session_{s_id}\"\n",
    "        save_model(model, model_name)\n",
    "        \n",
    "       \n",
    "    # Save comparison metrics for each session as a CSV\n",
    "    metrics_df = pull()\n",
    "    metrics_filename = f\"metrics/{dataset}_{feature}_{feature_set}_{transport}_train_raw_metrics_session_{s_id}.csv\"\n",
    "    metrics_df.to_csv(metrics_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
