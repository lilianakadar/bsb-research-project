{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pycaret.classification import setup, compare_models\n",
    "from pycaret.classification import tune_model\n",
    "from pycaret.classification import *\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pycaret.classification import load_model\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Me\n",
    "\n",
    "This file is used make prediction on small molecule dataset to predict their BBB permeability potential.\n",
    "It uses 3 models to predict influx, efflux, PAMPA and BBB labels. Influx, Efflux BBB models use a subset of molecular descriptors (CDK, Chemopy, PaDel), while PAMPA models are trained with Chemopy fingerprints. \n",
    "\n",
    "Every efflux positive molecules are filtered out.\n",
    "The following threshld are applied for the other 3 prediction to consider them positive:\n",
    "    influx: prediction score >= 0.7\n",
    "    PAMPA: prediction score >= 0.95\n",
    "    BBB: prediction score >= 0.95\n",
    "\n",
    "Composite score = influx score + PAMPA score + BBB score        Value range: 0-3\n",
    "transport score = prediction score if it is > threshold, otherwise 0    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the models for each prediction\n",
    "\n",
    "def define_models(t,f):\n",
    "\n",
    "    if t == \"influx\":\n",
    "        models =  ['XGBClassifier','RandomForestClassifier','GradientBoostingClassifier']\n",
    "\n",
    "    elif t ==\"efflux\":\n",
    "        models = ['LGBMClassifier','ExtraTreesClassifier','RandomForestClassifier']\n",
    "\n",
    "    elif t ==\"pampa\":\n",
    "        models = ['XGBClassifier','GradientBoostingClassifier','LogisticRegression']\n",
    "\n",
    "    elif t ==\"bbb\":\n",
    "        models = [\"LGBMClassifier\",\"XGBClassifier\",\"ExtraTreesClassifier\"]\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained classifier\n",
    "\n",
    "def load_the_model(t,f,model_name):\n",
    "\n",
    "    if f == \"submd\":\n",
    "        f2 = \"md\"\n",
    "    elif f ==\"subfp\":\n",
    "        f2 =\"fp\"\n",
    "\n",
    "    model_file = f'../model_building/blood_brain_barrier/combined/models_sub_{f2}/{t}/combined_{f}_{t}_{model_name}_session_16_{f}'\n",
    "    model = load_model(model_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction \n",
    "\n",
    "def make_prediction(model,df):\n",
    "    predictions = predict_model(model, data=df, raw_score=True)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the unseen data (the files used here are not provided on Github - change it to the compound data with molecular fingerprints, and with moelcular descriptors)\n",
    "\n",
    "df_submd = pd.read_feather('data/submd/small_mol_460k_submd.feather')\n",
    "df_submd.replace([np.inf, -np.inf], 0, inplace=True) #Remove infinitie values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset for only the prediction related information\n",
    "\n",
    "def clean_df_for_info(df,):\n",
    "\n",
    "    columns_to_keep = ['papyrus_SMILES','papyrus_inchi_key']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average prediction score\n",
    "\n",
    "def calculate_average(df):\n",
    "\n",
    "    avg_values = df.iloc[:, [-6, -4, -2]].mean(axis=1)\n",
    "\n",
    "    return avg_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction per transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean pred df: 460160\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write_feather() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m df_info_influx[column_maj_class] \u001b[38;5;241m=\u001b[39m counts\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Save prediction\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mdf_info_influx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbbb_pred_per_transport/\u001b[39;49m\u001b[38;5;132;43;01m{t}\u001b[39;49;00m\u001b[38;5;124;43m_prediction.feather\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lily\\.conda\\envs\\pyth9\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lily\\.conda\\envs\\pyth9\\lib\\site-packages\\pandas\\core\\frame.py:2795\u001b[0m, in \u001b[0;36mDataFrame.to_feather\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m   2770\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2771\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[0;32m   2772\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;124;03msupports custom indices e.g. `to_parquet`.\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeather_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_feather\n\u001b[1;32m-> 2795\u001b[0m to_feather(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lily\\.conda\\envs\\pyth9\\lib\\site-packages\\pandas\\io\\feather_format.py:93\u001b[0m, in \u001b[0;36mto_feather\u001b[1;34m(df, path, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeather must have string column names\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m     91\u001b[0m     path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     92\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m---> 93\u001b[0m     feather\u001b[38;5;241m.\u001b[39mwrite_feather(df, handles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: write_feather() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "#This is for influx, efflux and bbb\n",
    "\n",
    "#define transport\n",
    "t = 'efflux'    #transport - options: influx, efflux, bbb (DO NOT USE PAMPA!)\n",
    "f = \"submd\"     #feature \n",
    "\n",
    "models = define_models(t,f)\n",
    "\n",
    "df_info_influx = clean_df_for_info(df_submd)\n",
    "print(f'Clean pred df: {len(df_info_influx)}')\n",
    "\n",
    "#Iterate through models and make prediction\n",
    "for m in models:\n",
    "    classifier = load_the_model(t,f,m)\n",
    "    predictions = make_prediction(classifier,df_submd)\n",
    "\n",
    "    #Gather prediction and prediction score and add them to the prediction related dataframe\n",
    "    pred_classes = predictions['prediction_label'].values\n",
    "    probability_score= predictions['prediction_score_1'].values\n",
    "\n",
    "    column_mod_score=f'{m}_pred_score_{t}_{f}'\n",
    "    df_info_influx[column_mod_score] = probability_score\n",
    "\n",
    "    column_mod_class = f'{m}_pred_class_{t}_{f}'\n",
    "    df_info_influx[column_mod_class] = pred_classes\n",
    "\n",
    "#calculate average pred.score\n",
    "pred_score_means = calculate_average(df_info_influx)\n",
    "column_score_mean = f'{t}_pred_score_final'\n",
    "df_info_influx[column_score_mean] = pred_score_means\n",
    "\n",
    "#Majority vote\n",
    "counts = df_info_influx.iloc[:, [-6, -4, -2]].apply(lambda x: (x == 1).sum(), axis=1)\n",
    "column_maj_class =f'status_{t}'\n",
    "df_info_influx[column_maj_class] = counts.apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "#Save prediction\n",
    "file_name = f'bbb_pred_per_transport/{t}_prediction.feather'\n",
    "df_info_influx.to_feather(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This if for PAMPA prediction only\n",
    "#do not change the 't' and 'f' variable\n",
    "\n",
    "t = \"pampa\"  #transport\n",
    "f=\"subfp\"   #feature\n",
    "\n",
    "models = define_models(t)\n",
    "\n",
    "\n",
    "df_info_pampa = pd.DataFrame()\n",
    "\n",
    "for i in range(1,48):\n",
    "    file_name = f'data/subfp/small_mols_papyrus_460k_chemopy_{i}.feather'\n",
    "    df_tmp=pd.read_feather(file_name)\n",
    "\n",
    "    df_info_pampa_tmp = clean_df_for_info(df_tmp)\n",
    "\n",
    "\n",
    "    for m in models:\n",
    "        classifier = load_the_model(t,f,m)\n",
    "        predictions = make_prediction(classifier,df_tmp)\n",
    "\n",
    "        pred_classes = predictions['prediction_label'].values\n",
    "        probability_score= predictions['prediction_score_1'].values\n",
    "\n",
    "        column_mod_score=f'{m}_pred_score_{t}_{f}'\n",
    "        df_info_pampa_tmp[column_mod_score] = probability_score\n",
    "\n",
    "        column_mod_class = f'{m}_pred_class_{t}_{f}'\n",
    "        df_info_pampa_tmp[column_mod_class] = pred_classes\n",
    "\n",
    "\n",
    "    #calculate average pred.score\n",
    "    pred_score_means = calculate_average(df_info_pampa_tmp)\n",
    "    column_score_mean = f'{t}_pred_score_final'\n",
    "    df_info_pampa_tmp[column_score_mean] = pred_score_means\n",
    "\n",
    "    #majority vote\n",
    "    counts = df_info_pampa_tmp.iloc[:, [-6, -4, -2]].apply(lambda x: (x == 1).sum(), axis=1)\n",
    "    column_maj_class =f'status_{t}'\n",
    "    df_info_pampa_tmp[column_maj_class] = counts.apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "    df_info_pampa = pd.concat([df_info_pampa,df_info_pampa_tmp],ignore_index=True)\n",
    "\n",
    "#Save the prediction\n",
    "df_info_pampa.to_feather(\"pred_per_transport/pampa_predictio.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further data manipulation, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the predictions\n",
    "\n",
    "influx_info = pd.read_feather('bbb_pred_per_transport/influx_prediction.feather')\n",
    "influx_info = influx_info.reset_index(drop=True)\n",
    "\n",
    "efflux_info = pd.read_feather('bbb_pred_per_transport/efflux_prediction.feather')\n",
    "efflux_info = efflux_info.reset_index(drop=True)\n",
    "\n",
    "pampa_info = pd.read_feather('bbb_pred_per_transport/pampa_prediction.feather')\n",
    "pampa_info = pampa_info.reset_index(drop=True)\n",
    "\n",
    "bbb_info = pd.read_feather('bbb_pred_per_transport/bbb_prediction.feather')\n",
    "bbb_info = bbb_info.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data check\n",
    "# (efflux_info.papyrus_SMILES.sort_values().values == bbb_info.papyrus_SMILES.sort_values().values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply efflux filter: remove every efflux positive compound\n",
    "#remove duplicates\n",
    "\n",
    "data_noefflux = (pd.concat([influx_info.sort_values('papyrus_SMILES'), efflux_info.sort_values('papyrus_SMILES'), pampa_info.sort_values('papyrus_SMILES'), bbb_info.sort_values('papyrus_SMILES')], axis=1)\n",
    "                   .drop_duplicates(subset=\"papyrus_inchi_key\")\n",
    "                   .query(\"status_efflux == 0\")\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_noefflux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No efflux filter\n",
    "#remove duplicates\n",
    "\n",
    "data_withefflux = (pd.concat([influx_info.sort_values('papyrus_SMILES'), efflux_info.sort_values('papyrus_SMILES'), pampa_info.sort_values('papyrus_SMILES'), bbb_info.sort_values('papyrus_SMILES')], axis=1)\n",
    "                   .drop_duplicates(subset=\"papyrus_inchi_key\")\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define if from this point on which dataset you want to work with: after efflux filter, or without efflux filter\n",
    "predictions = data_noefflux.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composite Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define how the composite score is calculated, set thresholds\n",
    "\n",
    "def calculate_comp_score(df):\n",
    "\n",
    "    if df['influx_pred_score_final'] >= 0.7:\n",
    "        influx = df['influx_pred_score_final'] * df['status_influx']\n",
    "    else:\n",
    "        influx = 0\n",
    "    \n",
    "    if df['pampa_pred_score_final'] >= 0.95:\n",
    "        pampa = df['pampa_pred_score_final'] * df['status_pampa']\n",
    "    else: \n",
    "        pampa =0\n",
    "\n",
    "    if df['bbb_pred_score_final']  >= 0.95:\n",
    "        bbb = df['bbb_pred_score_final'] * df['status_bbb']\n",
    "    else:\n",
    "        bbb = 0\n",
    "\n",
    "    score = influx + pampa + bbb\n",
    "\n",
    "    return  score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the composite score\n",
    "predictions['composite_score'] = predictions.apply(calculate_comp_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222906\n"
     ]
    }
   ],
   "source": [
    "#Sort molecules by the composite scre: descending\n",
    "\n",
    "df_pred_sorted = predictions.sort_values(by='composite_score', ascending=False)\n",
    "print(len(df_pred_sorted))\n",
    "\n",
    "#Save the ranked prediction\n",
    "df_pred_sorted.to_csv('efflux_neg_small_molecules_bbb_screening_ranked.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset for better readability\n",
    "#!!! This step removed predicted classes and prediction scores\n",
    "\n",
    "columns_to_keep = ['papyrus_SMILES','papyrus_inchi_key','composite_score']\n",
    "df_clean = predictions[columns_to_keep]\n",
    "\n",
    "df_clean.to_csv(\"efflux_neg_small_molecules_bbb_screening_ranked_clean.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hits with composite_score >= 1.5: 44575\n"
     ]
    }
   ],
   "source": [
    "#Count BBB-permeability positive hits (threshold = 1.5)\n",
    "#NOTE: if you have not applied the efflux filter previously, this might contain efflux positive compounds !!!\n",
    "\n",
    "count_above_1_5 = df_clean[df_clean['composite_score'] >= 1.5].shape[0]\n",
    "\n",
    "print(f\"Number of hits with composite_score >= 1.5: {count_above_1_5}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_positives = df_clean[df_clean['composite_score'] >= 1.5]\n",
    "\n",
    "final_positives= final_positives.reset_index(drop=True)\n",
    "final_positives.to_csv('small_mol_bbb_pos_15_efflux_neg.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
