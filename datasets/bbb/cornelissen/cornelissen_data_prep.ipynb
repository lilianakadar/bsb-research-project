{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import inchi\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from chembl_structure_pipeline import standardizer as ChEMBL_standardizer\n",
    "from papyrus_structure_pipeline import standardize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw Cornelissen et al (2021) data\n",
    "df_raw = pd.read_csv('Cornelissen_master_file.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column names\n",
    "df_raw.rename(columns={'Status_Influx': 'status_influx',\n",
    "                       'Status_Efflux': 'status_efflux',\n",
    "                       'Status_PAMPA': 'status_pampa',\n",
    "                       'Status_BBB': 'status_bbb'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Papyrus Standardization\n",
    "\n",
    "def create_sd_smiles(sd_mol):\n",
    "    try:\n",
    "        standardized_smiles =  Chem.MolToSmiles(sd_mol)\n",
    "        return standardized_smiles\n",
    "    except Exception as e:\n",
    "        print(f\"An sd_smiles error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "#Create InChI keys from standardized molecules\n",
    "def mol_to_inchi_key(sd_mol):\n",
    "    if sd_mol is not None:\n",
    "\n",
    "        inchi_str = inchi.MolToInchi(sd_mol)\n",
    "        inchi_key = inchi.InchiToInchiKey(inchi_str)\n",
    "    else:\n",
    "        inchi_key = None   \n",
    "    return inchi_key\n",
    "\n",
    "def standardize_molecule(mol):\n",
    "    standardized_mol =  standardize(mol,raise_error=False )\n",
    "    return standardized_mol\n",
    "\n",
    "#Standardize \n",
    "\n",
    "def standardize_workflow(df_raw):\n",
    "    for i in range(0,len(df_raw)):\n",
    "        smiles =df_raw.at[i,'ParentSmiles']\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        sd_mol =  standardize_molecule(mol)\n",
    "        sd_smiles = create_sd_smiles(sd_mol)\n",
    "        sd_inchi_key = mol_to_inchi_key(sd_mol)\n",
    "        df_raw.at[i,'papyrus_SMILES'] = sd_smiles\n",
    "        df_raw.at[i,'papyrus_inchi_key'] = sd_inchi_key\n",
    "\n",
    "    print(f'df length after standardization: {len(df_raw)}')\n",
    "\n",
    "    #Save the dataset with standardized information\n",
    "    df_raw.to_csv('datasets/cornelissen_all_papyrus_standardized.csv', index=False)\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing inchi key\n",
    "\n",
    "def missing_inchi(df_raw):\n",
    "    smiles_nan = df_raw['papyrus_SMILES'].isna().sum()\n",
    "    inchikey_nan =df_raw['papyrus_inchi_key'].isna().sum()\n",
    "    print(f'DB length: {len(df_raw)},        SMILES nan: {smiles_nan},        inchi key nan: {inchikey_nan}')\n",
    "\n",
    "    #Remove rows with missing inchikey\n",
    "    df_valid_inchi= df_raw[((df_raw['papyrus_inchi_key'].notna()))]\n",
    "    print('-----remove missing inchikey----')\n",
    "    print(f'updated length: {len(df_valid_inchi)}')\n",
    "\n",
    "    return df_valid_inchi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create connectivity inchi column\n",
    "\n",
    "def inchi_first_part(inchi):\n",
    "    return inchi.split('-')[0]\n",
    "\n",
    "def create_connectivity_inchi(df):\n",
    "    df['inchi_connectivity'] = df['papyrus_inchi_key'].apply(inchi_first_part)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset for transport information\n",
    "\n",
    "def filter_transport(df,t):\n",
    "    status_col = f'status_{t}'\n",
    "    df = df.dropna(subset=[status_col])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "\n",
    "def remove_duplicates(df,t):\n",
    "    print(f'length: {len(df)}')\n",
    "    inchi_un = df['inchi_connectivity'].nunique()\n",
    "    print(f'unique_inchi: {inchi_un}')\n",
    "\n",
    "    status_col=f\"status_{t}\"\n",
    "    unique_counts = df.groupby('inchi_connectivity')[status_col].nunique()\n",
    "    duplicates_diff_class = unique_counts[unique_counts > 1].index\n",
    "\n",
    "    print(f'Contradicting duplicates: {len(duplicates_diff_class)}')\n",
    "\n",
    "    #Remove duplicates\n",
    "    df = df[~(df['inchi_connectivity'].isin(duplicates_diff_class))]\n",
    "    print(len(df))\n",
    "    print(df['inchi_connectivity'].nunique())\n",
    "\n",
    "    df=df.drop_duplicates(subset=['inchi_connectivity'], keep=\"first\").reset_index(drop=True)\n",
    "    #df['inchi_stereo'].values_counts\n",
    "    \n",
    "    df.to_csv('datasets/cornelissen_{t}_no_duplicates.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all the preprocessing to create dataset per transport\n",
    "df = df_raw\n",
    "\n",
    "transports =[\"influx\",'efflux','pampa','bbb']\n",
    "\n",
    "df_sd = standardize_workflow(df)        #Standardize molecules\n",
    "df_valid = missing_inchi(df_sd)         #Check for missing inchi key\n",
    "df_connectivity_inchi = create_connectivity_inchi(df_valid) #Create connectivity inchi column\n",
    "\n",
    "#Create separate datasets per transport mechanism\n",
    "for t in transports:\n",
    "    df_transport_filter = filter_transport(df_connectivity_inchi,t)     #filter for transport mechanism\n",
    "\n",
    "    df_no_duplicates = remove_duplicates(df_connectivity_inchi,t)       #remove duplicates\n",
    "    file_name = f'datasets/cornelissen_{t}.csv'                         #provide file name\n",
    "    df_no_duplicates.to_csv(file_name)                              #save datasets into csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename column\n",
    "\n",
    "def rename_col(df):\n",
    "    df.rename(columns={'TestValTrain_Influx': 'TrainTestVal_influx',\n",
    "                       'TestValTrain_Efflux': 'TrainTestVal_efflux',\n",
    "                       'TestValTrain_PAMPA': 'TrainTestVal_pampa',\n",
    "                       'TestValTrain_BBB': 'TrainTestVal_bbb'}, inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtered dataset: to keep only Train, or only Test/Val compounds\n",
    "\n",
    "def filter_dataset(df,dataset,transport):\n",
    "    filter_col=f\"TrainTestVal_{transport}\"\n",
    "\n",
    "    if dataset==\"train\":\n",
    "        filtered_df = df[df[filter_col] == 'Train']\n",
    "        \n",
    "    elif dataset == \"testval\":\n",
    "        filtered_df = df[df[filter_col].isin(['Test', 'Val'])]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the train-test split for all transport\n",
    "\n",
    "transports = ['influx','efflux','pampa','bbb']\n",
    "datasets = ['testval',\"train\"]\n",
    "\n",
    "df_raw = df_no_duplicates\n",
    "\n",
    "for t in transports:\n",
    "    for dataset in datasets:\n",
    "        df = rename_col(df_raw)         #Rename columns\n",
    "        df_filtered = filter_dataset(df,dataset,t)  #Filter for transport, and dataset type\n",
    "\n",
    "        save_file=f'datasets/cornelissen_{t}_{dataset}_raw.csv' #Define raw file name\n",
    "        df_filtered.to_csv(save_file,index=True)        #Save rawd ataset into csv\n",
    "\n",
    "        print(f'{t} - {dataset}: {len(df_filtered)}')       #print info for process check\n",
    "\n",
    "        stat_col =f'status_{t}'         \n",
    "        nan_values = df_filtered[stat_col].isna().any()   #Check that only rows with such transport information are present\n",
    "\n",
    "        if nan_values:\n",
    "            print(f'{stat_col} in {dataset} dataset contains NaN values.')\n",
    "        else:\n",
    "            print(f'{stat_col} in {dataset} dataset contains NO NaN values.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
