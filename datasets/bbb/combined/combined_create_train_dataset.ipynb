{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import inchi\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the files that we want to merge: k-train, c-train\n",
    "\n",
    "def load_files(transport):\n",
    "    c_train_file = f'../cornelissen_data_prep/datasets/cornelissen_{transport}_train_raw.csv'\n",
    "    c_train = pd.read_csv(c_train_file, index_col=0)\n",
    "    \n",
    "    k_train_file = f'../kadar_data_prep/train_data/kadar_{transport}_train.csv'\n",
    "    k_train = pd.read_csv(k_train_file, index_col=0)\n",
    "\n",
    "    return c_train, k_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code the classes: 1 for positive, 0 for negative class\n",
    "\n",
    "def fix_class(df,t):\n",
    "    status_col = f'status_{t}'\n",
    "\n",
    "    if t == \"influx\":\n",
    "        df[status_col] = df[status_col].replace({'Substrate': 1.0, 'Non-substrate': 0.0})\n",
    "    elif t == \"efflux\":\n",
    "        df[status_col] = df[status_col].replace({'Substrate': 1.0, 'Non-substrate': 0.0})\n",
    "    elif t == \"pampa\":\n",
    "        df[status_col] = df[status_col].replace({'high': 1.0, 'low': 0.0})\n",
    "    elif t ==\"bbb\":\n",
    "        df[status_col] = df[status_col].replace({'BBB+': 1.0, \"BBB-\": 0.0})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat datasets\n",
    "\n",
    "def concat_dfs(df1,df2):\n",
    "    df = pd.concat([df1,df2], ignore_index=True, join=\"inner\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all contradicting duplicates, and ensure all the other molecules are present only once\n",
    "\n",
    "def remove_duplicates(df,t):\n",
    "    status_col = f'status_{t}'\n",
    "    \n",
    "    print(f'----{t}----')\n",
    "    print(f'length: {len(df)}')\n",
    "    inchi_un = df['inchi_connectivity'].nunique()\n",
    "    print(f'unique_inchi: {inchi_un}')\n",
    "\n",
    "\n",
    "    unique_counts = df.groupby('inchi_connectivity')[status_col].nunique()\n",
    "    duplicates_diff_class = unique_counts[unique_counts > 1].index\n",
    "\n",
    "    print(f'Contradicting duplicates: {len(duplicates_diff_class)}')\n",
    "\n",
    "    #Remove duplicates\n",
    "    df = df[~(df['inchi_connectivity'].isin(duplicates_diff_class))]\n",
    "    print(f'After contradicting removed: {len(df)}')\n",
    "\n",
    "    df=df.drop_duplicates(subset=['inchi_connectivity'], keep=\"first\").reset_index(drop=True)\n",
    "    print(f'After duplicated removed: {len(df)}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define transports\n",
    "transports = ['influx','efflux','pampa','bbb']\n",
    "\n",
    "#Run the functions to create the combined training datasets\n",
    "for t in transports:\n",
    "    c_train_raw, k_train_raw = load_files(t)        #Load the files for merge\n",
    "    k_train = fix_class(k_train_raw,t)              #Code the classes with 0 and 1\n",
    "    df_combined = concat_dfs(c_train_raw,k_train)   #Concat k-train and c-train\n",
    "    df = remove_duplicates(df_combined,t)           #Deduplicate\n",
    "    display_jsonf = df.reset_index(drop=True)       #Reset indexing\n",
    "\n",
    "    file_name=f'datasets/combined_{t}_train_raw.csv'    #Define name for saving file\n",
    "    df.to_csv(file_name,index=True)                 #Save file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
