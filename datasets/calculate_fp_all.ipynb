{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>]\n",
      "[ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[<class 'str'>, <class 'float'>, <class 'bool'>]\n",
      "[ForwardRef('Period'), ForwardRef('Timestamp'), ForwardRef('Timedelta')]\n",
      "[ForwardRef('Period'), ForwardRef('Timestamp'), ForwardRef('Timedelta'), ForwardRef('Interval')]\n",
      "[<class 'str'>, <class 'float'>, <class 'bool'>, ForwardRef('Period'), ForwardRef('Timestamp'), ForwardRef('Timedelta'), ForwardRef('Interval'), <class 'numpy.datetime64'>, <class 'numpy.timedelta64'>, <class 'datetime.datetime'>]\n",
      "[ForwardRef('Timestamp'), <class 'datetime.datetime'>, <class 'numpy.datetime64'>, <class 'numpy.int64'>, <class 'float'>, <class 'str'>]\n",
      "[ForwardRef('Timedelta'), <class 'datetime.timedelta'>, <class 'numpy.timedelta64'>, <class 'numpy.int64'>, <class 'float'>, <class 'str'>]\n",
      "[<class 'str'>, <class 'datetime.tzinfo'>]\n",
      "[<class 'str'>, <class 'int'>]\n",
      "[typing.Hashable, typing.Sequence[typing.Hashable]]\n",
      "[<class 'str'>, <class 'float'>, <class 'bool'>, typing.List, typing.Dict]\n",
      "[<class 'str'>, <class 'float'>, <class 'bool'>, typing.List, typing.Dict, <class 'NoneType'>]\n",
      "[<class 'str'>, ForwardRef('BaseOffset')]\n",
      "[ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series'), typing.List, <class 'range'>]\n",
      "[<class 'int'>, ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, <class 'numpy.random._generator.Generator'>, <class 'numpy.random.bit_generator.BitGenerator'>, <class 'numpy.random.mtrand.RandomState'>]\n",
      "[<class 'str'>, <class 'complex'>, <class 'bool'>, <class 'object'>]\n",
      "[<class 'str'>, <class 'numpy.dtype'>, typing.Type[typing.Union[str, complex, bool, object]]]\n",
      "[ForwardRef('ExtensionDtype'), <class 'str'>, <class 'numpy.dtype'>, typing.Type[typing.Union[str, complex, bool, object]]]\n",
      "[ForwardRef('ExtensionDtype'), ForwardRef('npt.DTypeLike')]\n",
      "[ForwardRef('ExtensionDtype'), <class 'str'>, <class 'numpy.dtype'>, typing.Type[typing.Union[str, complex, bool, object]], typing.Dict[typing.Hashable, typing.Union[ForwardRef('ExtensionDtype'), str, numpy.dtype, typing.Type[typing.Union[str, complex, bool, object]]]]]\n",
      "[<class 'numpy.dtype'>, ForwardRef('ExtensionDtype')]\n",
      "[<class 'bool'>, typing.List[typing.Hashable], typing.List[typing.List[typing.Hashable]], typing.Dict[typing.Hashable, typing.List[typing.Hashable]]]\n",
      "[typing.Mapping[typing.Any, typing.Hashable], typing.Callable[[typing.Any], typing.Hashable]]\n",
      "[ForwardRef('Series'), ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[typing.Callable[[ForwardRef('Series')], typing.Union[ForwardRef('Series'), ForwardRef('ExtensionArray'), numpy.ndarray, ForwardRef('Index')]], <class 'NoneType'>]\n",
      "[ForwardRef('Index'), ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[typing.Callable[[ForwardRef('Index')], typing.Union[ForwardRef('Index'), ForwardRef('ExtensionArray'), numpy.ndarray, ForwardRef('Series')]], <class 'NoneType'>]\n",
      "[typing.Callable, <class 'str'>]\n",
      "[typing.Callable, <class 'str'>, typing.List[typing.Union[typing.Callable, str]]]\n",
      "[typing.Callable, <class 'str'>, typing.List[typing.Union[typing.Callable, str]], typing.Dict[typing.Hashable, typing.Union[typing.Callable, str, typing.List[typing.Union[typing.Callable, str]]]]]\n",
      "[ForwardRef('Series'), ForwardRef('DataFrame'), ForwardRef('GroupBy'), ForwardRef('SeriesGroupBy'), ForwardRef('DataFrameGroupBy'), ForwardRef('BaseWindow'), ForwardRef('Resampler')]\n",
      "[<class 'str'>, ForwardRef('PathLike[str]')]\n",
      "[typing.Dict[str, typing.Any], <class 'NoneType'>]\n",
      "[typing.Literal['infer', 'gzip', 'bz2', 'zip', 'xz', 'zstd', 'tar'], typing.Dict[str, typing.Any]]\n",
      "[typing.Literal['infer', 'gzip', 'bz2', 'zip', 'xz', 'zstd', 'tar'], typing.Dict[str, typing.Any], <class 'NoneType'>]\n",
      "[typing.List[typing.Callable], typing.Tuple[typing.Callable, ...], typing.Mapping[typing.Union[str, int], typing.Callable]]\n",
      "[<class 'str'>, typing.Callable, ForwardRef('EngFormatter')]\n",
      "[<class 'str'>, <class 'int'>, typing.Sequence[typing.Union[str, int]], typing.Mapping[typing.Hashable, typing.Union[str, int]]]\n",
      "[ForwardRef('ArrayManager'), ForwardRef('SingleArrayManager'), ForwardRef('BlockManager'), ForwardRef('SingleBlockManager')]\n",
      "[ForwardRef('SingleArrayManager'), ForwardRef('SingleBlockManager')]\n",
      "[ForwardRef('ArrayManager'), ForwardRef('BlockManager')]\n",
      "[<class 'int'>, <class 'numpy.integer'>]\n",
      "[<class 'slice'>, typing.List[int], <class 'numpy.ndarray'>]\n",
      "[<class 'int'>, <class 'numpy.integer'>, <class 'slice'>, typing.List[int], <class 'numpy.ndarray'>]\n",
      "[<class 'int'>, <class 'numpy.integer'>, <class 'slice'>, typing.List[int], <class 'numpy.ndarray'>, typing.Tuple[typing.Union[int, numpy.integer, slice, typing.List[int], numpy.ndarray], typing.Union[int, numpy.integer, slice, typing.List[int], numpy.ndarray]]]\n",
      "[typing.Literal['left', 'right'], typing.Literal['both', 'neither']]\n",
      "[<class 'datetime.datetime'>, ForwardRef('NaTType')]\n",
      "[typing.Literal['ignore', 'raise'], typing.Literal['coerce']]\n",
      "[<class 'pandas.util.version.InfinityType'>, <class 'pandas.util.version.NegativeInfinityType'>]\n",
      "[<class 'pandas.util.version.InfinityType'>, <class 'pandas.util.version.NegativeInfinityType'>, typing.Tuple[str, int]]\n",
      "[<class 'pandas.util.version.InfinityType'>, <class 'pandas.util.version.NegativeInfinityType'>, <class 'int'>, <class 'str'>]\n",
      "[<class 'pandas.util.version.InfinityType'>, <class 'pandas.util.version.NegativeInfinityType'>, <class 'int'>, <class 'str'>, typing.Tuple[typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str], str], typing.Tuple[pandas.util.version.NegativeInfinityType, typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str]]]\n",
      "[<class 'pandas.util.version.NegativeInfinityType'>, typing.Tuple[typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str, typing.Tuple[typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str], str], typing.Tuple[pandas.util.version.NegativeInfinityType, typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str]]], ...]]\n",
      "[typing.Tuple[int, typing.Tuple[int, ...], typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[pandas.util.version.NegativeInfinityType, typing.Tuple[typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str, typing.Tuple[typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str], str], typing.Tuple[pandas.util.version.NegativeInfinityType, typing.Union[pandas.util.version.InfinityType, pandas.util.version.NegativeInfinityType, int, str]]], ...]]], typing.Tuple[int, typing.Tuple[str, ...]]]\n",
      "[ForwardRef('Period'), ForwardRef('Timestamp'), ForwardRef('Timedelta'), <class 'pandas._libs.tslibs.nattype.NaTType'>]\n",
      "[typing.Mapping[str, str], typing.Iterable[str]]\n",
      "[<class 'pandas._libs.interval.Interval'>, <class 'float'>]\n",
      "[<class 'str'>, <class 'pandas._libs.missing.NAType'>]\n",
      "[typing.List, typing.Tuple, ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[<class 'float'>, <class 'str'>]\n",
      "[<class 'float'>, <class 'str'>, <class 'datetime.datetime'>]\n",
      "[<class 'float'>, <class 'str'>, <class 'datetime.datetime'>, typing.List, typing.Tuple, ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[typing.List[typing.Union[float, str]], typing.Tuple[typing.Union[float, str], ...], ForwardRef('ExtensionArray'), <class 'numpy.ndarray'>, ForwardRef('Index'), ForwardRef('Series')]\n",
      "[<class 'pandas.core.tools.datetimes.FulldatetimeDict'>, ForwardRef('DataFrame')]\n",
      "[typing.Hashable, typing.List[typing.Hashable], typing.Callable[[typing.Hashable], typing.Hashable], typing.List[typing.Callable[[typing.Hashable], typing.Hashable]], typing.Mapping[typing.Hashable, typing.Hashable]]\n",
      "[<class 'str'>, typing.Callable[..., typing.Any]]\n",
      "[<class 'str'>, typing.Iterable[str]]\n",
      "[typing.Tuple[int, int], typing.Tuple[int]]\n",
      "[<class 'pyparsing.results.ParseResults'>, typing.Sequence[pyparsing.results.ParseResults]]\n",
      "[typing.Callable[[], typing.Any], typing.Callable[[pyparsing.results.ParseResults], typing.Any], typing.Callable[[int, pyparsing.results.ParseResults], typing.Any], typing.Callable[[str, int, pyparsing.results.ParseResults], typing.Any]]\n",
      "[typing.Callable[[], bool], typing.Callable[[pyparsing.results.ParseResults], bool], typing.Callable[[int, pyparsing.results.ParseResults], bool], typing.Callable[[str, int, pyparsing.results.ParseResults], bool]]\n",
      "[typing.Callable[[str, int, ForwardRef('ParserElement'), bool], NoneType], <class 'NoneType'>]\n",
      "[typing.Callable[[str, int, int, ForwardRef('ParserElement'), pyparsing.results.ParseResults, bool], NoneType], <class 'NoneType'>]\n",
      "[typing.Callable[[str, int, ForwardRef('ParserElement'), Exception, bool], NoneType], <class 'NoneType'>]\n",
      "[<class 'pyparsing.results.ParseResults'>, <class 'Exception'>]\n",
      "[<class 'int'>, <class 'NoneType'>]\n",
      "[typing.Set[str], <class 'str'>]\n",
      "[<class 'str'>, <class 'pathlib.Path'>, <class 'typing.TextIO'>]\n",
      "[<class 'str'>, typing.List[str]]\n",
      "[ForwardRef('ParserElement'), <class 'str'>]\n",
      "[ForwardRef('ParserElement'), <class 'str'>, <class 'NoneType'>]\n",
      "[typing.Callable[[str, pyparsing.results.ParseResults], str], <class 'NoneType'>]\n",
      "[<class 'typing.TextIO'>, <class 'NoneType'>]\n",
      "[<class 'typing.TextIO'>, <class 'pathlib.Path'>, <class 'str'>]\n",
      "[<enum 'RegexFlag'>, <class 'int'>]\n",
      "[<class 'pyparsing.core.ParserElement'>, <class 'str'>]\n",
      "[<class 'str'>, <class 'pyparsing.core.ParserElement'>]\n",
      "[<class 'pyparsing.core.ParserElement'>, <class 'str'>, <class 'NoneType'>]\n",
      "[<class 'pyparsing.core.ParserElement'>, <class 'NoneType'>]\n",
      "[typing.Iterable[str], <class 'str'>]\n",
      "[<class 'pyparsing.core.ParserElement'>, <class 'str'>, typing.Tuple[typing.Union[pyparsing.core.ParserElement, str], typing.Union[pyparsing.core.ParserElement, str]]]\n",
      "[typing.Callable[[], typing.Any], typing.Callable[[pyparsing.results.ParseResults], typing.Any], typing.Callable[[int, pyparsing.results.ParseResults], typing.Any], typing.Callable[[str, int, pyparsing.results.ParseResults], typing.Any], <class 'NoneType'>]\n",
      "[typing.Tuple[typing.Union[pyparsing.core.ParserElement, str, typing.Tuple[typing.Union[pyparsing.core.ParserElement, str], typing.Union[pyparsing.core.ParserElement, str]]], int, pyparsing.helpers.OpAssoc, typing.Union[typing.Callable[[], typing.Any], typing.Callable[[pyparsing.results.ParseResults], typing.Any], typing.Callable[[int, pyparsing.results.ParseResults], typing.Any], typing.Callable[[str, int, pyparsing.results.ParseResults], typing.Any], NoneType]], typing.Tuple[typing.Union[pyparsing.core.ParserElement, str, typing.Tuple[typing.Union[pyparsing.core.ParserElement, str], typing.Union[pyparsing.core.ParserElement, str]]], int, pyparsing.helpers.OpAssoc]]\n",
      "[<class 'str'>, os.PathLike[str]]\n",
      "[<class 'module'>, <class 'str'>]\n",
      "[<class 'module'>, <class 'str'>, <class 'NoneType'>]\n",
      "[<class 'importlib_resources.abc.ResourceReader'>, <class 'NoneType'>]\n",
      "[<class 'seaborn.external.version.InfinityType'>, <class 'seaborn.external.version.NegativeInfinityType'>]\n",
      "[<class 'seaborn.external.version.InfinityType'>, <class 'seaborn.external.version.NegativeInfinityType'>, typing.Tuple[str, int]]\n",
      "[<class 'seaborn.external.version.InfinityType'>, <class 'seaborn.external.version.NegativeInfinityType'>, <class 'int'>, <class 'str'>]\n",
      "[<class 'seaborn.external.version.InfinityType'>, <class 'seaborn.external.version.NegativeInfinityType'>, <class 'int'>, <class 'str'>, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str], str], typing.Tuple[seaborn.external.version.NegativeInfinityType, typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str]]]\n",
      "[<class 'seaborn.external.version.NegativeInfinityType'>, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str], str], typing.Tuple[seaborn.external.version.NegativeInfinityType, typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str]]], ...]]\n",
      "[typing.Tuple[int, typing.Tuple[int, ...], typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, typing.Tuple[str, int]], typing.Union[seaborn.external.version.NegativeInfinityType, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str], str], typing.Tuple[seaborn.external.version.NegativeInfinityType, typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str]]], ...]]], typing.Tuple[int, typing.Tuple[str, ...]]]\n",
      "[typing.Tuple[str, int], <class 'NoneType'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'typing.SupportsInt'>]\n",
      "[<class 'seaborn.external.version.NegativeInfinityType'>, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str, typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str], str], typing.Tuple[seaborn.external.version.NegativeInfinityType, typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str]]], ...], <class 'NoneType'>]\n",
      "[typing.Tuple[typing.Union[seaborn.external.version.InfinityType, seaborn.external.version.NegativeInfinityType, int, str]], <class 'NoneType'>]\n",
      "[<class 'float'>, <class 'numpy.floating'>, <class 'numpy.integer'>]\n",
      "[typing.Callable, <class 'NoneType'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'datetime.date'>, <class 'datetime.datetime'>, <class 'datetime.timedelta'>, <class 'bool'>, <class 'complex'>, <class 'pandas._libs.tslibs.timestamps.Timestamp'>, <class 'pandas._libs.tslibs.timedeltas.Timedelta'>]\n",
      "[<class 'pandas.core.series.Series'>, <class 'pandas.core.indexes.base.Index'>, <class 'numpy.ndarray'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'datetime.date'>, <class 'datetime.datetime'>, <class 'datetime.timedelta'>, <class 'bool'>, <class 'complex'>, <class 'pandas._libs.tslibs.timestamps.Timestamp'>, <class 'pandas._libs.tslibs.timedeltas.Timedelta'>, <class 'pandas.core.series.Series'>, <class 'pandas.core.indexes.base.Index'>, <class 'numpy.ndarray'>, <class 'NoneType'>]\n",
      "[typing.List[typing.Union[str, bytes, datetime.date, datetime.datetime, datetime.timedelta, bool, complex, pandas._libs.tslibs.timestamps.Timestamp, pandas._libs.tslibs.timedeltas.Timedelta, pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray, NoneType]], <class 'pandas.core.indexes.base.Index'>, <class 'NoneType'>]\n",
      "[<class 'pandas.core.frame.DataFrame'>, typing.Mapping[collections.abc.Hashable, typing.Union[pandas.core.series.Series, pandas.core.indexes.base.Index, numpy.ndarray]], <class 'NoneType'>]\n",
      "[<class 'collections.abc.Iterable'>, <class 'NoneType'>]\n",
      "[<class 'float'>, <class 'NoneType'>]\n",
      "[typing.Tuple[typing.Optional[float], typing.Optional[float]], <class 'matplotlib.colors.Normalize'>, <class 'NoneType'>]\n",
      "[<class 'str'>, <class 'list'>, <class 'dict'>, <class 'matplotlib.colors.Colormap'>, <class 'NoneType'>]\n",
      "[<class 'dict'>, <class 'list'>, <class 'NoneType'>]\n",
      "[typing.Tuple[float, float], typing.List[float], typing.Dict[typing.Any, float], <class 'NoneType'>]\n",
      "[typing.Tuple[chemopy.geo_opt.MopacResultDir, str], <class 'NoneType'>]\n",
      "[<class 'rdkit.Chem.rdchem.Mol'>, <class 'NoneType'>]\n",
      "[<class 'str'>, <class 'bytes'>]\n",
      "[<class 'str'>, <class 'float'>]\n",
      "[<enum 'ns'>, <class 'int'>]\n",
      "[<class 'natsort.utils.SupportsDunderLT'>, <class 'natsort.utils.SupportsDunderGT'>]\n",
      "[typing.Tuple[bytes], typing.Tuple[typing.Tuple[bytes]]]\n",
      "[typing.Tuple[typing.Any, ...], typing.Tuple[typing.Tuple[typing.Any, ...], ...]]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'float'>, <class 'int'>]\n",
      "[<class 'str'>, <class 'pathlib.PurePath'>]\n",
      "[typing.Match, <class 'NoneType'>]\n",
      "[<class 'natsort.utils.SupportsDunderLT'>, <class 'natsort.utils.SupportsDunderGT'>, <class 'NoneType'>]\n",
      "[typing.Callable[[typing.Any], typing.Union[natsort.utils.SupportsDunderLT, natsort.utils.SupportsDunderGT, NoneType]], <class 'NoneType'>]\n",
      "[typing.Callable[[typing.Union[str, pathlib.PurePath]], typing.Union[typing.Tuple[typing.Any, ...], typing.Tuple[typing.Tuple[typing.Any, ...], ...]]], typing.Callable[[typing.Union[str, pathlib.PurePath]], typing.Tuple[typing.Union[typing.Tuple[typing.Any, ...], typing.Tuple[typing.Tuple[typing.Any, ...], ...]], ...]]]\n",
      "[<class 'natsort.utils.SupportsDunderLT'>, <class 'natsort.utils.SupportsDunderGT'>, <class 'NoneType'>, typing.Any]\n",
      "[typing.Callable[[~T], typing.Union[natsort.utils.SupportsDunderLT, natsort.utils.SupportsDunderGT, NoneType]], <class 'NoneType'>]\n",
      "[<class 'bytes'>, typing.List[bytes]]\n",
      "[typing.List[charset_normalizer.models.CharsetMatch], <class 'NoneType'>]\n",
      "[<class 'int'>, <class 'str'>]\n",
      "[ForwardRef('CharsetMatch'), <class 'NoneType'>]\n",
      "[<class 'bytes'>, <class 'bytearray'>]\n",
      "[typing.List[str], <class 'NoneType'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'os.PathLike'>]\n",
      "[<class 'os.PathLike'>, <class 'str'>, <class 'typing.BinaryIO'>, <class 'bytes'>]\n",
      "[<class 'str'>, <class 'float'>, <class 'NoneType'>]\n",
      "[<class 'bytes'>, <class 'str'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'bytearray'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.dh.DHPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x25519.X25519PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x448.X448PublicKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.dh.DHPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x25519.X25519PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x448.X448PrivateKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x25519.X25519PublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.x448.X448PublicKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.ciphers.modes.Mode'>, <class 'NoneType'>]\n",
      "[<class 'cryptography.hazmat.primitives.ciphers.modes.ModeWithNonce'>, <class 'cryptography.hazmat.primitives.ciphers.modes.ModeWithTweak'>, <class 'NoneType'>, <class 'cryptography.hazmat.primitives.ciphers.modes.ECB'>, <class 'cryptography.hazmat.primitives.ciphers.modes.ModeWithInitializationVector'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PublicKey'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey'>]\n",
      "[<class 'ipaddress.IPv4Address'>, <class 'ipaddress.IPv6Address'>, <class 'ipaddress.IPv4Network'>, <class 'ipaddress.IPv6Network'>]\n",
      "[<class 'cryptography.hazmat.primitives.hashes.SHA224'>, <class 'cryptography.hazmat.primitives.hashes.SHA256'>, <class 'cryptography.hazmat.primitives.hashes.SHA384'>, <class 'cryptography.hazmat.primitives.hashes.SHA512'>, <class 'cryptography.hazmat.primitives.hashes.SHA3_224'>, <class 'cryptography.hazmat.primitives.hashes.SHA3_256'>, <class 'cryptography.hazmat.primitives.hashes.SHA3_384'>, <class 'cryptography.hazmat.primitives.hashes.SHA3_512'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPublicKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey'>]\n",
      "[<class 'bytes'>, typing.Callable[..., bytes]]\n",
      "[<class 'bytes'>, <class 'NoneType'>]\n",
      "[ForwardRef('X509'), <class 'NoneType'>]\n",
      "[ForwardRef('_CRLInternal'), <class 'cryptography.x509.base.CertificateRevocationList'>]\n",
      "[<class 'str'>, <class 'bytes'>, <class 'os.PathLike'>, <class 'NoneType'>]\n",
      "[typing.Sequence[OpenSSL.crypto.X509], <class 'NoneType'>]\n",
      "[<class 'bytes'>, typing.Callable[..., bytes], <class 'NoneType'>]\n",
      "[typing.Tuple[OpenSSL.crypto.Revoked, ...], <class 'NoneType'>]\n",
      "[<class 'OpenSSL.crypto.X509'>, <class 'NoneType'>]\n",
      "[<class 'OpenSSL.crypto.PKey'>, <class 'NoneType'>]\n",
      "[typing.Tuple[OpenSSL.crypto.X509, ...], <class 'NoneType'>]\n",
      "[typing.Iterable[OpenSSL.crypto.X509], <class 'NoneType'>]\n",
      "[<class 'cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.dsa.DSAPrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed25519.Ed25519PrivateKey'>, <class 'cryptography.hazmat.primitives.asymmetric.ed448.Ed448PrivateKey'>]\n",
      "[<class 'cryptography.x509.base.Certificate'>, <class 'cryptography.hazmat.primitives.serialization.pkcs12.PKCS12Certificate'>]\n",
      "[<class 'bool'>, <class 'str'>]\n",
      "[<class 'datetime.datetime'>, <class 'NoneType'>]\n",
      "[<class 'rdkit.Chem.rdchem.Mol'>, typing.List[rdkit.Chem.rdchem.Mol]]\n",
      "[<class 'dict'>, typing.List[dict]]\n",
      "[<class 'int'>, <class 'float'>]\n",
      "[<class 'PaDEL_pywrapper.descriptor.Descriptor'>, <class 'PaDEL_pywrapper.descriptor.Fingerprint'>]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import inchi\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from jCompoundMapper_pywrapper import JCompoundMapper\n",
    "from chemopy import ChemoPy\n",
    "from chemopy import Fingerprint\n",
    "from CDK_pywrapper import CDK, FPType\n",
    "from PaDEL_pywrapper import PaDEL\n",
    "from PaDEL_pywrapper.descriptor import SubstructureFPCount\n",
    "from PaDEL_pywrapper.descriptor import KlekotaRothFPCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the compounds for which we are creating molecular fingerprints\n",
    "\n",
    "def load_compounds(file_name):\n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate dataset length specific variabels for calculation optimization\n",
    "\n",
    "def calculate_variables(df):\n",
    "\n",
    "    comp_number = len(df)\n",
    "    step = int(comp_number / 10) +1\n",
    "    job_number = 7 \n",
    "    chunk = int(step / 7) +1 \n",
    "\n",
    "    return comp_number, step, job_number, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of SMILES\n",
    "\n",
    "def generate_smiles(df):\n",
    "    smiles_list = df['papyrus_SMILES'].tolist()\n",
    "\n",
    "    return smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate molecules from SMILES\n",
    "\n",
    "def generate_mols(smiles_list):\n",
    "    mols = []\n",
    "    smiles_to_fix = []\n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            mols.append(mol)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with SMILES string {smiles}\")\n",
    "            continue \n",
    "\n",
    "\n",
    "    #check for None Mol objects\n",
    "    for i, mol in enumerate(mols):\n",
    "        if mol is None:\n",
    "            print(f\"Error with SMILES string at index {i}: {smiles_list[i]}\")\n",
    "            smiles_to_fix.append(i)\n",
    "            continue\n",
    "\n",
    "    return mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add hydrogens to the molecules\n",
    "\n",
    "def add_hydrogens(mols):\n",
    "\n",
    "    mols_H = []\n",
    "\n",
    "    for mol in mols:\n",
    "        mol_H = Chem.AddHs(mol)\n",
    "        mols_H.append(mol_H)\n",
    "\n",
    "    return mols_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate molecular fingerprints\n",
    "\n",
    "From this point on we are creating and saving the different mol. fingerprints (function are named accordingly) one-by one into a raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemopy_all(mols_H,comp_number,step,file_name_prefix):\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating Chemopy fingerprints...')\n",
    "\n",
    "    for mol in mols_H:\n",
    "        new_data = Fingerprint.get_all_fps(mol)\n",
    "        fp = fp.append([new_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_chemopy_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDK pub\n",
    "\n",
    "def cdk_pub(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    cdk = CDK(fingerprint=FPType.PubchemFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Pubchem fingerprints...')\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_pub_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_fp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK FP fingerprints\n",
    "    cdk = CDK(fingerprint=FPType.FP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_FP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_fp_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_ext(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "  \n",
    "    cdk = CDK(fingerprint=FPType.ExtFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_ext_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_graph(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Graph fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.GraphFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Graph fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_graph_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK MACCS fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.MACCSFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_maccs_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_sub(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Sub fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.SubFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Sub fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_sub_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_kr(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Klekota-Roth fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.KRFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_kr_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Atom pair 2D fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.AP2DFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_AP2DFP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_ap2d_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_hybrid(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK hybrid fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.HybridFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Hybrid fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_hybrid_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_lingo(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "    #CDK LINGO fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.LingoFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Lingo fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_lingo_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_sp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK shortest path fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.SPFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_SP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_sp_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_circ(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK circular fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.CircFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Circ fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_circ_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_estate(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK E-State fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.EStateFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_EState fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_estate_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padel_subcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#PaDel Substructrure + Counts\n",
    "\n",
    "    fp_type = SubstructureFPCount\n",
    "\n",
    "    padel = PaDEL([fp_type], ignore_3D=False)\n",
    "\n",
    "    print(f'Calculating PaDEL_SubstructureCount fingerprints...')\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = padel.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_padel_subcount.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padel_krcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#PaDel KlekotaRoth + Couns\n",
    "\n",
    "\n",
    "    fp_type = KlekotaRothFPCount\n",
    "\n",
    "\n",
    "    padel = PaDEL([fp_type], ignore_3D=False)\n",
    "\n",
    "    print(f'Calculating PaDEL_KlekotaRothCount fingerprints...')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = padel.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_padel_krcount.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_dfs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper(Fingerprint.DFS)\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper DFS fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_dfs.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('AP2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper AP2d fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ap2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_asp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ASP')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ASP...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_asp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_cats2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('CATS2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper CATS2D fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_cats2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_at2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('AT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_at2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_22(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('PHAP2POINT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper PHAP2POINT2D fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_22.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_32(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('PHAP3POINT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_PHAP3POINT2D...')\n",
    "        \n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_32.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ecfp(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ECFP')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ECFP...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ecfp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ecvar(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ECFPVariant')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ECFPVariant...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ecvar.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_lstar(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('LSTAR')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_lstar.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_shed(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('SHED')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_SHED...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_shed.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_rad2(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('RAD2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_RAD2D...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_rad2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('MACCS')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_maccs.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingerprints calculating functions ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mol_fps(file_path,file_name_prefix,file_save):\n",
    "\n",
    "    df = load_compounds(file_path)\n",
    "    comp_number, step, job_number, chunk = calculate_variables(df)\n",
    "\n",
    "    smiles_list= generate_smiles(df)\n",
    "    mols = generate_mols(smiles_list)\n",
    "    mols_H = add_hydrogens(mols)\n",
    "\n",
    "\n",
    "    #Generating fingerprints\n",
    "\n",
    "    cdk_ap2d_fp =cdk_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_circ_fp = cdk_circ(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_estate_fp = cdk_estate(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_ext_fp = cdk_ext(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_fp_fp =cdk_fp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_graph_fp =cdk_graph(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_hybrid_fp = cdk_hybrid(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_kr_fp = cdk_kr(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_lingo_fp = cdk_lingo(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_maccs_fp = cdk_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_pub_fp = cdk_pub(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_sp_fp = cdk_sp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_sub_fp = cdk_sub(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "\n",
    "    chemopy_fp = chemopy_all(mols_H,comp_number,step,file_name_prefix)\n",
    "\n",
    "    jcm_22_fp = jcm_22(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_32_fp =jcm_32(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ap2d_fp = jcm_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_asp_fp = jcm_asp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_at2d_fp = jcm_at2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_cats2d_fp = jcm_cats2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_dfs_fp=jcm_dfs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ecfp_fp = jcm_ecfp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ecvar_fp = jcm_ecvar(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_lstar_fp = jcm_lstar(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_maccs_fp = jcm_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_rad2d_fp = jcm_rad2(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_shed_fp = jcm_shed(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    \n",
    "    padel_subcount_fp = padel_subcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    padel_krcount_fp = padel_krcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "\n",
    "    #Add prefixes\n",
    "\n",
    "    cdk_ap2d_fp = cdk_ap2d_fp.add_prefix('cdk_ap2d - ')\n",
    "    cdk_circ_fp = cdk_circ_fp.add_prefix('cdk_circ - ')\n",
    "    cdk_estate_fp = cdk_estate_fp.add_prefix('cdk_estate - ')\n",
    "    cdk_ext_fp = cdk_ext_fp.add_prefix('cdk_ext - ')\n",
    "    cdk_fp_fp = cdk_fp_fp.add_prefix('cdk_fp - ')\n",
    "    cdk_graph_fp = cdk_graph_fp.add_prefix('cdk_grap - ')\n",
    "    cdk_hybrid_fp = cdk_hybrid_fp.add_prefix('cdk_hybrid - ')\n",
    "    cdk_kr_fp = cdk_kr_fp.add_prefix('cdk_kr - ')\n",
    "    cdk_lingo_fp = cdk_lingo_fp.add_prefix ('cdk_lingo - ')\n",
    "    cdk_maccs_fp = cdk_maccs_fp.add_prefix('cdk_maccs - ')\n",
    "    cdk_pubc_fp = cdk_pub_fp.add_prefix('cdk_pubc - ')\n",
    "    cdk_sp_fp = cdk_sp_fp.add_prefix('cdk_sp - ')\n",
    "    cdk_sub_fp = cdk_sub_fp.add_prefix('cdk_sub - ')\n",
    "\n",
    "    chemopy_fp = chemopy_fp.add_prefix('chemopy - ')\n",
    "\n",
    "    jcm_22_fp = jcm_22_fp.add_prefix('jcm_22 - ')\n",
    "    jcm_32_fp = jcm_32_fp.add_prefix('jcm_32 - ')\n",
    "    jcm_ap2d_fp = jcm_ap2d.add_prefix('jcm_ap2d - ')\n",
    "    jcm_asp_fp = jcm_asp_fp.add_prefix('jcm_asp - ')\n",
    "    jcm_at2d_fp = jcm_at2d_fp.add_prefix('jcm_at2d - ')\n",
    "    jcm_cats2d_fp = jcm_cats2d_fp.add_prefix('jcm_cats2d - ')\n",
    "    jcm_dfs_fp = jcm_dfs_fp.add_prefix('jcm_dfs - ')\n",
    "    jcm_ecfp_fp = jcm_ecfp_fp.add_prefix('jcm_ecfp - ')\n",
    "    jcm_ecvar_fp = jcm_ecvar_fp.add_prefix('jcm_ecvar - ')\n",
    "    jcm_lstar_fp = jcm_lstar_fp.add_prefix(' jcm_lstar - ')\n",
    "    jcm_maccs_fp = jcm_maccs_fp.add_prefix('jcm_maccs - ')\n",
    "    jcm_rad2d_fp = jcm_rad2d_fp.add_prefix('jcm_rad2d - ')\n",
    "    jcm_shed_fp = jcm_shed_fp.add_prefix('jcm_shed - ')\n",
    "\n",
    "    padel_subcount_fp = padel_subcount_fp.add_prefix('padel_subcount - ')\n",
    "    padel_krcount_fp = padel_krcount_fp.add_prefix('padel_krcount - ')\n",
    "    \n",
    "    #Merging\n",
    "\n",
    "    compounds_fingerprints = df.merge(cdk_ap2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_circ_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_estate_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_ext_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_fp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_graph_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_hybrid_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_kr_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_lingo_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_maccs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_pubc_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_sp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_sub_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(chemopy_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_22_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_32_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ap2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_asp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_at2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_cats2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_dfs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ecfp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ecvar_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_lstar_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_maccs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_rad2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_shed_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(padel_subcount_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(padel_krcount_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    len(compounds_fingerprints)\n",
    "    \n",
    "    compounds_fingerprints.to_csv(file_save, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide file specific variables: one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify source file: path to the compounds with column \"papyrus_SMILES\" containing SMILES\n",
    "\n",
    "file_path = f'../blood_brain_barrier/kadar_data_prep/val_data/kadar_influx_val.csv'\n",
    "\n",
    "#Sppecify file_prefix to save intermediate files: raw molecular descriptors\n",
    "\n",
    "file_name_prefix = f'kadar_fp/influx/kadar_val_influx_val'\n",
    "\n",
    "#Specify final file path for saving\n",
    "\n",
    "file_save = f\"kadar_fp/influx/kadar_val_influx_fp_all.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating CDK_AP2DFP fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c2ceded9054887838a7e93d359cba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Circ fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f27d095fe64ef3933975c0f8bcc393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_EState fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e6d9673a5643b2b437b1e10f9820cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5078d63668ff492bbb8d2b65c6b31d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_FP fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0ac1c943a456aa7b35a58921f13a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Graph fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a46895c98534c58bf85eefa0107b188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Hybrid fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d8e427711e4cd3bb683538b9e364c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d6a11937344f919ec13fd5af32f007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Lingo fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d195f7cad304852987bfb072984bd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6c30e89b824942ae9496dda8ea6681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Pubchem fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323ac134ce9f453db34328d1b1608730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_SP fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92caa2699530431bbbbaae3456006571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating CDK_Sub fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b9fcfa59ba48c3abb118a8cd69171c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating Chemopy fingerprints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n",
      "C:\\Users\\Lily\\AppData\\Local\\Temp\\ipykernel_4184\\3826568385.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  fp = fp.append([new_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 18\n",
      "Number of rows with NaN values: 0\n",
      "Calculating JCompoundMapper PHAP2POINT2D fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51c8b95d24445e59d1b63f12118fef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n",
      "Calculating JCM_PHAP3POINT2D...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d15e6c68024a23bc40eb19c7d46d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n",
      "Calculating JCompoundMapper AP2d fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a923c7e45f4437ba1afa1050bd4627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n",
      "Calculating JCM_ASP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ed62d94ee453c9c9f8d7214e4aa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f776416fca48fd9d811f7363d102e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n",
      "Calculating JCompoundMapper CATS2D fingerprints...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d054742fd264b97a4847ad37d058185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Number of rows with NaN values: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Fingerprint' has no attribute 'DFS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Run the functions to generate mol.descs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgenerate_mol_fps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_name_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_save\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 35\u001b[0m, in \u001b[0;36mgenerate_mol_fps\u001b[1;34m(file_path, file_name_prefix, file_save)\u001b[0m\n\u001b[0;32m     33\u001b[0m jcm_at2d_fp \u001b[38;5;241m=\u001b[39m jcm_at2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n\u001b[0;32m     34\u001b[0m jcm_cats2d_fp \u001b[38;5;241m=\u001b[39m jcm_cats2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n\u001b[1;32m---> 35\u001b[0m jcm_dfs_fp\u001b[38;5;241m=\u001b[39m\u001b[43mjcm_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmols_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcomp_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_name_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjob_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m jcm_ecfp_fp \u001b[38;5;241m=\u001b[39m jcm_ecfp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n\u001b[0;32m     37\u001b[0m jcm_ecvar_fp \u001b[38;5;241m=\u001b[39m jcm_ecvar(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m, in \u001b[0;36mjcm_dfs\u001b[1;34m(mols_H, comp_number, step, file_name_prefix, chunk, job_number)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjcm_dfs\u001b[39m(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n\u001b[1;32m----> 3\u001b[0m     jcm \u001b[38;5;241m=\u001b[39m JCompoundMapper(\u001b[43mFingerprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDFS\u001b[49m)\n\u001b[0;32m      6\u001b[0m     fp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating JCompoundMapper DFS fingerprints...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Fingerprint' has no attribute 'DFS'"
     ]
    }
   ],
   "source": [
    "#Run the functions to generate mol.descs\n",
    "generate_mol_fps(file_path,file_name_prefix,file_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
