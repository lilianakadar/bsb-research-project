{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import inchi\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from jCompoundMapper_pywrapper import JCompoundMapper\n",
    "from chemopy import ChemoPy\n",
    "from chemopy import Fingerprint\n",
    "from CDK_pywrapper import CDK, FPType\n",
    "from PaDEL_pywrapper import PaDEL\n",
    "from PaDEL_pywrapper.descriptor import SubstructureFPCount\n",
    "from PaDEL_pywrapper.descriptor import KlekotaRothFPCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the compounds for which we are creating molecular fingerprints\n",
    "\n",
    "def load_compounds(file_name):\n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate dataset length specific variabels for calculation optimization\n",
    "\n",
    "def calculate_variables(df):\n",
    "\n",
    "    comp_number = len(df)\n",
    "    step = int(comp_number / 10) +1\n",
    "    \n",
    "    job_number = 7 \n",
    "    chunk = int(step / 7) +1 \n",
    "\n",
    "    return comp_number, step, job_number, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of SMILES\n",
    "\n",
    "def generate_smiles(df):\n",
    "    smiles_list = df['papyrus_SMILES'].tolist()\n",
    "\n",
    "    return smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate molecules from SMILES\n",
    "\n",
    "def generate_mols(smiles_list):\n",
    "    mols = []\n",
    "    smiles_to_fix = []\n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            mols.append(mol)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with SMILES string {smiles}\")\n",
    "            continue \n",
    "\n",
    "\n",
    "    #check for None Mol objects\n",
    "    for i, mol in enumerate(mols):\n",
    "        if mol is None:\n",
    "            print(f\"Error with SMILES string at index {i}: {smiles_list[i]}\")\n",
    "            smiles_to_fix.append(i)\n",
    "            continue\n",
    "\n",
    "    return mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add hydrogens to the molecules\n",
    "\n",
    "def add_hydrogens(mols):\n",
    "\n",
    "    mols_H = []\n",
    "\n",
    "    for mol in mols:\n",
    "        mol_H = Chem.AddHs(mol)\n",
    "        mols_H.append(mol_H)\n",
    "\n",
    "    return mols_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate molecular fingerprints\n",
    "\n",
    "From this point on we are creating and saving the different mol. fingerprints (function are named accordingly) one-by one into a raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemopy_all(mols_H,comp_number,step,file_name_prefix):\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating Chemopy fingerprints...')\n",
    "\n",
    "    for mol in mols_H:\n",
    "        new_data = Fingerprint.get_all_fps(mol)\n",
    "        fp = fp.append([new_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_chemopy_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDK pub\n",
    "\n",
    "def cdk_pub(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    cdk = CDK(fingerprint=FPType.PubchemFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Pubchem fingerprints...')\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_pub_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_fp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK FP fingerprints\n",
    "    cdk = CDK(fingerprint=FPType.FP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_FP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_fp_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_ext(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "  \n",
    "    cdk = CDK(fingerprint=FPType.ExtFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_ext_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_graph(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Graph fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.GraphFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Graph fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_graph_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK MACCS fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.MACCSFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_maccs_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_sub(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Sub fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.SubFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Sub fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_sub_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_kr(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Klekota-Roth fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.KRFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_kr_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK Atom pair 2D fingerprints\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.AP2DFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_AP2DFP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_ap2d_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_hybrid(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "    #CDK hybrid fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.HybridFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Hybrid fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_hybrid_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_lingo(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "    #CDK LINGO fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.LingoFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Lingo fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_lingo_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_sp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK shortest path fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.SPFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_SP fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_sp_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_circ(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK circular fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.CircFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_Circ fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_circ_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdk_estate(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#CDK E-State fingerprints\n",
    "\n",
    "\n",
    "    cdk = CDK(fingerprint=FPType.EStateFP)\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating CDK_EState fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = cdk.calculate(mols_H[i:i+step], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_cdk_estate_fp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padel_subcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#PaDel Substructrure + Counts\n",
    "\n",
    "    fp_type = SubstructureFPCount\n",
    "\n",
    "    padel = PaDEL([fp_type], ignore_3D=False)\n",
    "\n",
    "    print(f'Calculating PaDEL_SubstructureCount fingerprints...')\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = padel.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_padel_subcount.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padel_krcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "#PaDel KlekotaRoth + Couns\n",
    "\n",
    "\n",
    "    fp_type = KlekotaRothFPCount\n",
    "\n",
    "\n",
    "    padel = PaDEL([fp_type], ignore_3D=False)\n",
    "\n",
    "    print(f'Calculating PaDEL_KlekotaRothCount fingerprints...')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = padel.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_padel_krcount.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_dfs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper(Fingerprint.DFS)\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper DFS fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_dfs.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('AP2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper AP2d fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ap2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_asp(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ASP')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ASP...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_asp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_cats2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('CATS2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper CATS2D fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_cats2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_at2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('AT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_at2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_22(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('PHAP2POINT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print(f'Calculating JCompoundMapper PHAP2POINT2D fingerprints...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_22.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_32(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('PHAP3POINT2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_PHAP3POINT2D...')\n",
    "        \n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_32.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ecfp(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ECFP')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ECFP...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ecfp.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_ecvar(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('ECFPVariant')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_ECFPVariant...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_ecvar.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_lstar(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('LSTAR')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_lstar.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_shed(mols_H,comp_number,step,file_name_prefix, chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('SHED')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_SHED...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_shed.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_rad2(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('RAD2D')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    print('Calculating JCM_RAD2D...')\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_rad2d.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jcm_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number):\n",
    "\n",
    "    jcm = JCompoundMapper('MACCS')\n",
    "\n",
    "\n",
    "    fp = pd.DataFrame()\n",
    "\n",
    "    for i in tqdm(range(0,comp_number,step), smoothing=1.0):\n",
    "        new_data = jcm.calculate(mols_H[i:i+500], show_banner=False, njobs=job_number, chunksize=chunk)\n",
    "        fp = pd.concat([fp, new_data], ignore_index=True)\n",
    "\n",
    "    file_name = f'{file_name_prefix}_raw_jcm_maccs.csv'\n",
    "    fp.to_csv(file_name, index=True)\n",
    "    print(f'Length: {len(fp)}')\n",
    "        #-----\n",
    "    nan_per_column = fp.isna().sum()\n",
    "    rows_with_nan = fp.isna().any(axis=1).sum()\n",
    "\n",
    "    print(f'Number of rows with NaN values: {rows_with_nan}')\n",
    "\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fingerprints calculating functions ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mol_fps(file_path,file_name_prefix,file_save):\n",
    "\n",
    "    df = load_compounds(file_path)\n",
    "    comp_number, step, job_number, chunk = calculate_variables(df)\n",
    "\n",
    "    smiles_list= generate_smiles(df)\n",
    "    mols = generate_mols(smiles_list)\n",
    "    mols_H = add_hydrogens(mols)\n",
    "\n",
    "\n",
    "    #Generating fingerprints\n",
    "\n",
    "    cdk_ap2d_fp =cdk_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_circ_fp = cdk_circ(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_estate_fp = cdk_estate(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_ext_fp = cdk_ext(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_fp_fp =cdk_fp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_graph_fp =cdk_graph(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_hybrid_fp = cdk_hybrid(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_kr_fp = cdk_kr(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_lingo_fp = cdk_lingo(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_maccs_fp = cdk_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_pub_fp = cdk_pub(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_sp_fp = cdk_sp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    cdk_sub_fp = cdk_sub(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "\n",
    "    chemopy_fp = chemopy_all(mols_H,comp_number,step,file_name_prefix)\n",
    "\n",
    "    jcm_22_fp = jcm_22(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_32_fp =jcm_32(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ap2d_fp = jcm_ap2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_asp_fp = jcm_asp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_at2d_fp = jcm_at2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_cats2d_fp = jcm_cats2d(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_dfs_fp=jcm_dfs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ecfp_fp = jcm_ecfp(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_ecvar_fp = jcm_ecvar(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_lstar_fp = jcm_lstar(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_maccs_fp = jcm_maccs(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_rad2d_fp = jcm_rad2(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    jcm_shed_fp = jcm_shed(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    \n",
    "    padel_subcount_fp = padel_subcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "    padel_krcount_fp = padel_krcount(mols_H,comp_number,step,file_name_prefix,chunk,job_number)\n",
    "\n",
    "    #Add prefixes\n",
    "\n",
    "    cdk_ap2d_fp = cdk_ap2d_fp.add_prefix('cdk_ap2d - ')\n",
    "    cdk_circ_fp = cdk_circ_fp.add_prefix('cdk_circ - ')\n",
    "    cdk_estate_fp = cdk_estate_fp.add_prefix('cdk_estate - ')\n",
    "    cdk_ext_fp = cdk_ext_fp.add_prefix('cdk_ext - ')\n",
    "    cdk_fp_fp = cdk_fp_fp.add_prefix('cdk_fp - ')\n",
    "    cdk_graph_fp = cdk_graph_fp.add_prefix('cdk_grap - ')\n",
    "    cdk_hybrid_fp = cdk_hybrid_fp.add_prefix('cdk_hybrid - ')\n",
    "    cdk_kr_fp = cdk_kr_fp.add_prefix('cdk_kr - ')\n",
    "    cdk_lingo_fp = cdk_lingo_fp.add_prefix ('cdk_lingo - ')\n",
    "    cdk_maccs_fp = cdk_maccs_fp.add_prefix('cdk_maccs - ')\n",
    "    cdk_pubc_fp = cdk_pub_fp.add_prefix('cdk_pubc - ')\n",
    "    cdk_sp_fp = cdk_sp_fp.add_prefix('cdk_sp - ')\n",
    "    cdk_sub_fp = cdk_sub_fp.add_prefix('cdk_sub - ')\n",
    "\n",
    "    chemopy_fp = chemopy_fp.add_prefix('chemopy - ')\n",
    "\n",
    "    jcm_22_fp = jcm_22_fp.add_prefix('jcm_22 - ')\n",
    "    jcm_32_fp = jcm_32_fp.add_prefix('jcm_32 - ')\n",
    "    jcm_ap2d_fp = jcm_ap2d.add_prefix('jcm_ap2d - ')\n",
    "    jcm_asp_fp = jcm_asp_fp.add_prefix('jcm_asp - ')\n",
    "    jcm_at2d_fp = jcm_at2d_fp.add_prefix('jcm_at2d - ')\n",
    "    jcm_cats2d_fp = jcm_cats2d_fp.add_prefix('jcm_cats2d - ')\n",
    "    jcm_dfs_fp = jcm_dfs_fp.add_prefix('jcm_dfs - ')\n",
    "    jcm_ecfp_fp = jcm_ecfp_fp.add_prefix('jcm_ecfp - ')\n",
    "    jcm_ecvar_fp = jcm_ecvar_fp.add_prefix('jcm_ecvar - ')\n",
    "    jcm_lstar_fp = jcm_lstar_fp.add_prefix(' jcm_lstar - ')\n",
    "    jcm_maccs_fp = jcm_maccs_fp.add_prefix('jcm_maccs - ')\n",
    "    jcm_rad2d_fp = jcm_rad2d_fp.add_prefix('jcm_rad2d - ')\n",
    "    jcm_shed_fp = jcm_shed_fp.add_prefix('jcm_shed - ')\n",
    "\n",
    "    padel_subcount_fp = padel_subcount_fp.add_prefix('padel_subcount - ')\n",
    "    padel_krcount_fp = padel_krcount_fp.add_prefix('padel_krcount - ')\n",
    "    \n",
    "    #Merging\n",
    "\n",
    "    compounds_fingerprints = df.merge(cdk_ap2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_circ_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_estate_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_ext_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_fp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_graph_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_hybrid_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_kr_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_lingo_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_maccs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_pubc_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_sp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(cdk_sub_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(chemopy_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_22_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_32_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ap2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_asp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_at2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_cats2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_dfs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ecfp_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_ecvar_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_lstar_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_maccs_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_rad2d_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(jcm_shed_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(padel_subcount_fp, left_index = True, right_index=True, how='outer')\n",
    "    compounds_fingerprints = compounds_fingerprints.merge(padel_krcount_fp, left_index = True, right_index=True, how='outer')\n",
    "\n",
    "    len(compounds_fingerprints)\n",
    "    \n",
    "    #Saving the merged, final file\n",
    "    compounds_fingerprints.to_csv(file_save, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide file specific variables: one-by-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify source file: path to the compounds with column \"papyrus_SMILES\" containing SMILES\n",
    "\n",
    "file_path = f'../blood_brain_barrier/kadar_data_prep/val_data/kadar_influx_val.csv'\n",
    "\n",
    "#Specify file_prefix to save intermediate files: raw molecular descriptors\n",
    "\n",
    "file_name_prefix = f'kadar_fp/influx/kadar_val_influx_val'\n",
    "\n",
    "#Specify final file path for saving\n",
    "\n",
    "file_save = f\"kadar_fp/influx/kadar_val_influx_fp_all.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the functions to generate mol.descs\n",
    "generate_mol_fps(file_path,file_name_prefix,file_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
